{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>polarity</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52332</th>\n",
       "      <td>b'Sat Jun 06 14:47:40 PDT 2009'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'@nextepisodenet KB: Now thanks to you I have...</td>\n",
       "      <td>b'marco1475'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52979</th>\n",
       "      <td>b'Thu Jun 18 12:44:23 PDT 2009'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'the merkat makes me all dizzy. '</td>\n",
       "      <td>b'gameOVERdose'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  date  polarity        query  \\\n",
       "52332  b'Sat Jun 06 14:47:40 PDT 2009'         4  b'NO_QUERY'   \n",
       "52979  b'Thu Jun 18 12:44:23 PDT 2009'         0  b'NO_QUERY'   \n",
       "\n",
       "                                                    text             user  \n",
       "52332  b'@nextepisodenet KB: Now thanks to you I have...     b'marco1475'  \n",
       "52979                 b'the merkat makes me all dizzy. '  b'gameOVERdose'  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_size = 100000\n",
    "ds = tfds.load('sentiment140', split='train', shuffle_files=True)\n",
    "df = tfds.as_dataframe(ds.take(df_size))\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    50021\n",
       "0    49979\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Note: Binary classification can be used. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bin_polarity'] = df['polarity'].apply(lambda x: 1 if x == 4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the RNN is that we take just the output of the final word (unless we're doing a bidirectional format, or a concatenated format where we concatenate the output of all the words)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating X values\n",
    "\n",
    "The text is in byte form, so we need to convert it to string form, and then use the split functionality to convert to a list. We ignore the first two characters which are a side effect of the conversion, and the last element in the string which is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns names: ['date', 'polarity', 'query', 'text', 'user', 'bin_polarity']\n"
     ]
    }
   ],
   "source": [
    "print(f'columns names: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separating the words into a list of words\n",
    "df['split_words'] = df['text'].apply(lambda x: str(x)[2:].split()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the length of words\n",
    "df['txt_length'] = df['split_words'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.8, 11.7]     21460\n",
       "(3.9, 7.8]      20917\n",
       "(11.7, 15.6]    17393\n",
       "(15.6, 19.5]    13669\n",
       "(19.5, 23.4]    12096\n",
       "(23.4, 27.3]     7499\n",
       "(-0.04, 3.9]     5696\n",
       "(27.3, 31.2]     1233\n",
       "(31.2, 35.1]       36\n",
       "(35.1, 39.0]        1\n",
       "Name: txt_length, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['txt_length'].value_counts(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Very few tweets are over 30 words, so we will create an RNN based on 30 words\n",
    "\n",
    "We now need to create a words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 30 #this is the value we use to specify how many words to consider in the tweet (first e.g. 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_words = 2000\n",
    "top_2k_words = list(df['split_words'].explode().value_counts()[:no_words].index)\n",
    "\n",
    "word_dict_2k = {}\n",
    "for idx, word in enumerate(top_2k_words):\n",
    "    word_dict_2k[idx+1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create additional values for 'word is over' and 'unknown'\n",
    "word_dict_2k[no_words+1] = 'word_over'\n",
    "word_dict_2k[0] = 'UNKNOWN-WORD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict_reversed = {}\n",
    "for key, value in word_dict_2k.items():\n",
    "    word_dict_reversed[value] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Our X values need to be 2000 x 30 x m, one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with a smaller set for now\n",
    "\n",
    "reduced_df = df.iloc[:df_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the words in numeric index format\n",
    "x = reduced_df['split_words'].apply(lambda x: np.array([word_dict_reversed[word] if word in word_dict_reversed.keys() else 0 for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''we're going to make all the x values the same length'''\n",
    "array_x = np.zeros((df_size,Tx)) #blank array to put them in\n",
    "for idx,arr in enumerate(x):\n",
    "    leng = len(arr) \n",
    "    #get length, and fork based on current size\n",
    "    if leng > Tx:\n",
    "        array_x[idx] = arr[:Tx] #just take first 30 values\n",
    "    elif leng < Tx:\n",
    "        array_x[idx] = np.append(arr,np.zeros(30-leng)+no_words+1) #append 30 minus the current length -1s\n",
    "#put examples on the columns\n",
    "array_x = array_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now one hot encode them'''\n",
    "one_hot_x_40k = np.zeros((no_words+2,Tx,df_size))\n",
    "for row_idx, row in enumerate(array_x):\n",
    "\n",
    "    for exam_idx, word_val in enumerate(row):\n",
    "\n",
    "        one_hot_x_40k[int(word_val),row_idx,exam_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 127., 2001.,    3., 1211.,    0., 2001., 1319.,    0., 2001.,\n",
       "         71., 2001., 2001., 2001.,  109., 2001.,   16., 2001., 2001.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_x[12,32:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We've used 2002 in order to incorporatate the 2001th word (end of word) and unknown index. We can need to create a mask which tells the machine to skip if the mask is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_x  = np.where(array_x==2001,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "OK so we have our x inputs. Mask x, shaped 30 x 40000, and the actual onehotencoded x values, of shape 2001, 30, 40000\n",
    "\n",
    "In the basic RNN, ignoring the last stage for a second, each cell has three weights matrixes.\n",
    "\n",
    "### Forward Prop\n",
    "\n",
    "WAa - the weights applied to the previous cells outputs\n",
    "\n",
    "WAx - the weights applied to the X values\n",
    "\n",
    "WaB - a bias term. \n",
    "\n",
    "We have to make a choice of how large each cell is, and then initialise the weights. We'll use a Xavier initialization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a cell size of 50\n",
    "cell_size = 50\n",
    "x_size = no_words+no_words #Xavier_initialization of weights.\n",
    "WAa = np.random.uniform(-(6)/(np.sqrt(cell_size*2)),(6)/((cell_size*2)),[cell_size,cell_size]) \n",
    "WAb = np.random.uniform(-(6)/(np.sqrt(cell_size+1)),(6)/((cell_size+1)),[cell_size,1]) \n",
    "WAx = np.random.uniform(-(6)/(np.sqrt(cell_size+x_size)),(6)/((cell_size+x_size)),[cell_size,x_size]) \n",
    "\n",
    "#We need a weights matrix too for later\n",
    "WYa = np.random.uniform(-(6)/((cell_size+1)),(6)/((cell_size+1)),[1,cell_size])\n",
    "WYb = np.random.uniform(-(6)/((1+1)),(6)/((1+1)),[1,1])\n",
    "\n",
    "weights_dict = {}\n",
    "weights_dict['WAa'] = WAa\n",
    "weights_dict['WAb'] = WAb\n",
    "weights_dict['WAx'] = WAx\n",
    "weights_dict['WYa'] = WYa\n",
    "weights_dict['WYb'] = WYb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's implement one run of forward prop in order to replicate it going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these allow us to set some default values\n",
    "a0 = np.zeros((cell_size,df_size))\n",
    "z_dict, a_dict = {}, {}\n",
    "a_dict[0] = a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''See notes from above - this is just implemented as a function'''\n",
    "def forward_prop(a_prev,  WAa, WAx, WAb, x, mask,time_period=0):\n",
    "    \n",
    "    # #z1 - the pre-tanh values of a0, bias and x1 times weights\n",
    "    z1 = WAa @ a_prev + WAx @ x[:,time_period,:] + WAb\n",
    "    \n",
    "    # #a1 either equals a1 if mask ==1 or equal tanh z1 if mask == - \n",
    "    a1 = mask[time_period].reshape(1,-1) * a_prev + (1-mask[time_period]) * np.tanh(z1)\n",
    "    \n",
    "    return a1, z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The idea here is to loop over the time periods and run forward prop each time. We will need to re-use a1 and z1 when we come back and do the back prop, so we will store them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cells_fw_prop(Tx = Tx, z_dict, a_dict, a0,WAa, WAx, WAb, x, mask):\n",
    "    a_prev = a0 #to have an initial value to call the forward_prop function\n",
    "    for i in range(Tx):\n",
    "        a_, z_ = forward_prop(a_prev=a_prev, time_period = i,x=x,mask = mask,WAa=WAa, WAx=WAx, WAb=WAb)\n",
    "        z_dict[i+1], a_dict[i+1] = z_, a_\n",
    "        a_prev = a_\n",
    "    #we end by returning the dictionary for z and a values throughout the time period\n",
    "    return z_dict, a_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Lets put it all together in a forward prop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Yz):\n",
    "        return (1/(1+np.exp(-Yz)))\n",
    "    \n",
    "def full_fw_prop(weights_dict, z_dict, a_dict, x, mask, Tx = Tx):\n",
    "    \n",
    "    # initialize a0 and dict values\n",
    "    a0 = np.zeros((cell_size, x.shape[2]))\n",
    "    a_dict[0] = a0\n",
    "    WAa, WAb, WAx, WYa, WYb = weights_dict.values()\n",
    "\n",
    "    #do the cell forward prop    \n",
    "    z_dict, a_dict = cells_fw_prop(Tx = Tx, z_dict=z_dict, a_dict=a_dict, a0=a0,WAa=WAa, WAx=WAx, WAb=WAb, x=x,mask=mask)\n",
    "\n",
    "    #extract last value and prediction\n",
    "    final_a = a_dict[Tx]\n",
    "    Yz = WYa @ final_a + WYb\n",
    "   \n",
    "    Ÿ = sigmoid(Yz)\n",
    "\n",
    "    return z_dict, a_dict, Yz, Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Back Propogation\n",
    "\n",
    "We can conveniently use the differentiated sigmoid function, which gives\n",
    "\n",
    "DL/DZ = Ÿ - Y\n",
    "\n",
    "From there we can initially back calculate the values of the final portion, giving values for DL/DA, DL/DWYa, DL/DWYb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn Y into an array\n",
    "Y = np.array(reduced_df['bin_polarity']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start by initializing the backprop dicts\n",
    "dA_dict, dZ_dict = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "OK let's create the relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_one_layer(prev_Loss, t, weights_dict,  a_dict,z_dict,  X, mask_x,learning_rate = 0.01,batch_size=400):\n",
    "    \n",
    "    WAa, WAb, WAx, WYa, WYb = weights_dict.values()\n",
    "    a_ = a_dict[t]\n",
    "    z_ = z_dict[t]\n",
    "    a__ = a_dict[t-1]\n",
    "    x_ = X[:,t-1,:]\n",
    "\n",
    "    #get dA and dZ as a base for the layer\n",
    "    dL_dA = prev_Loss #40000,50\n",
    "    dA_dZ = ((1-mask_x[t-1]) * (1-(np.tanh(z_)**2))).T #40000,50\n",
    "    \n",
    "    #get a previous\n",
    "    dZ_dAp = WAa #50x50\n",
    "    dA_dAp = mask_x[t-1].reshape(-1,1) + np.zeros_like((dL_dA)) + (dA_dZ @ dZ_dAp) #we create a mask of 1s everywhere here.\n",
    "    dL_dAp = dL_dA * dA_dAp\n",
    "\n",
    "    #differentiate with respect to weights\n",
    "    dZ_dWAa = a__ #50 x m\n",
    "    dL_dWAa = ((dL_dA * dA_dZ).T @ dZ_dWAa.T)/batch_size #weights dont matter if we are having a 0 on the mask\n",
    "\n",
    "    dZ_dWAb = np.zeros((batch_size,1))+1\n",
    "    dL_dWAb = ((dL_dA * dA_dZ).T @ dZ_dWAb)/batch_size\n",
    "\n",
    "    dZ_dWAx = x_\n",
    "    dL_dWAx = ((dL_dA * dA_dZ).T @ dZ_dWAx.T)/batch_size #weights dont matter if we are having a 0 on the mask\n",
    "    \n",
    "    #now update the weights based on the findings here.\n",
    "    return dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_full_swing(Ÿ, Y, Tx=Tx, weights_dict,  a_dict,z_dict,  X, mask_x,learning_rate = 0.01,batch_size = 100):\n",
    "    \n",
    "    '''initial loss function w.r.t Z'''\n",
    "    WAa, WAb, WAx, WYa, WYb = weights_dict.values()\n",
    "    dL_dZ = (Ÿ - Y).T #40000 x 1\n",
    "\n",
    "\n",
    "    # first order diffs\n",
    "    dZ_dA = WYa # 1 x 50\n",
    "    dZ_dWYa = a_dict[Tx] # 50 x 40000\n",
    "    dZ_dB = np.zeros_like(dL_dZ) + 1\n",
    "\n",
    "    #chain ruled diffs\n",
    "    dL_dA = dL_dZ @ dZ_dA\n",
    "    dL_dWYa = (dZ_dWYa @ dL_dZ).T / batch_size\n",
    "    dL_dWYb = (dZ_dB.T @ dL_dZ)/ batch_size\n",
    "    \n",
    "    weights_dict['WYa'] -= learning_rate * dL_dWYa\n",
    "    weights_dict['WYb'] -= learning_rate * dL_dWYb\n",
    "    \n",
    "    prev_loss = dL_dA\n",
    "    '''now go through the other functions'''\n",
    "    for t in reversed(range(1,Tx)):\n",
    "        #extract relevant differentials for updating backprop and also carrying on the backprop through the layers\n",
    "        dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp = back_prop_one_layer(prev_Loss=prev_loss, t=t, weights_dict = weights_dict,X=x,  a_dict=a_dict,z_dict=z_dict,mask_x=mask_x,batch_size=batch_size)\n",
    "        weights_dict['WAa'] -= learning_rate*dL_dWAa\n",
    "        weights_dict['WAx'] -= learning_rate*dL_dWAx\n",
    "        weights_dict['WAb'] -= learning_rate*dL_dWAb\n",
    "        prev_loss = dL_dAp\n",
    "        return dL_dZ, dZ_dA, dZ_dB, dL_dA, dL_dWYa, dL_dWYb, dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back_prop_full_swing(Ÿ=Ÿ, Y=Y, Tx=Tx, weights_dict=weights_dict,  a_dict=a_dict,z_dict=z_dict,  X=one_hot_x_40k, mask_x=mask_x,learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Putting it all together\n",
    "\n",
    "Let's now create a function which runs forward prop and backward prop in batches, and calculates the loss at each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.49\n",
      "Epoch: 0, loss: 0.7850219847952613\n",
      "Accuracy score: 0.5309452736318407\n",
      "Epoch: 200, loss: 0.689136217178594\n",
      "Accuracy score: 0.5364676616915423\n",
      "Epoch: 400, loss: 0.6982539029965139\n",
      "Accuracy score: 0.542960199004975\n",
      "Epoch: 600, loss: 0.6808523880735468\n",
      "Accuracy score: 0.544228855721393\n",
      "Epoch: 800, loss: 0.6758142577257107\n"
     ]
    }
   ],
   "source": [
    "'''set some initial parameters'''\n",
    "epochs = 1000\n",
    "cell_size = 100\n",
    "x_size = no_words+2\n",
    "\n",
    "''' initialize weights'''\n",
    "\n",
    "WAx = np.random.uniform(-1,1,[cell_size,x_size])\n",
    "WAb = np.random.uniform(-1,1,[cell_size,1])\n",
    "WAa = np.random.uniform(-0.1,0.1,[cell_size,cell_size])\n",
    "WYa = np.random.uniform(-0.25,0.25,[1,cell_size])\n",
    "WYb = np.random.uniform(-1,1,[1,1])\n",
    "\n",
    "weights_dict = {}\n",
    "weights_dict['WAa'] = WAa\n",
    "weights_dict['WAb'] = WAb\n",
    "weights_dict['WAx'] = WAx\n",
    "weights_dict['WYa'] = WYa\n",
    "weights_dict['WYb'] = WYb\n",
    "\n",
    "'''define additional dictionaries and characteristics'''\n",
    "z_dict, a_dict = {}, {}\n",
    "losses = []\n",
    "accs = []\n",
    "batches=500\n",
    "batch_size = int(df_size//batches) #so 400\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #run fw prop\n",
    "    '''create batching'''\n",
    "    \n",
    "\n",
    "    ''' This sections deals with batching'''\n",
    "    \n",
    "    round_ = epoch % batches #deals with if it's the first of a new set of batches\n",
    "    n, k = batch_size*round_, batch_size*(1 + round_)\n",
    "    x = one_hot_x_40k[:,:,n:k]\n",
    "    mask_x_ = mask_x[:,n:k]\n",
    "    y = Y[0,n:k]\n",
    "    \n",
    "    '''Forward prop'''\n",
    "    z_dict, a_dict, Yz, Ÿ = full_fw_prop(Tx = Tx, z_dict=z_dict, a_dict=a_dict, weights_dict = weights_dict, x=x,mask=mask_x_)\n",
    "    '''create loss metrics for reporting'''\n",
    "    loss = -np.sum(y*np.log(Ÿ) + (1-y)*np.log(1-Ÿ))/batch_size\n",
    "    accuracy = np.sum(y==np.where(Ÿ>0.5,1,0))/batch_size\n",
    "    losses.append(loss)\n",
    "    accs.append(accuracy)\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'Accuracy score: {np.mean(accs[(epoch-batch_size):])}')    \n",
    "        print(f'Epoch: {epoch}, loss: {loss}')\n",
    "    \n",
    "    #run backprop\n",
    "    dL_dZ, dZ_dA, dZ_dB, dL_dA, dL_dWYa, dL_dWYb, dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp = back_prop_full_swing(Ÿ=Ÿ, Y=y, Tx=Tx, weights_dict=weights_dict,  a_dict=a_dict,z_dict=z_dict,  X=x, mask_x=mask_x_,learning_rate = 0.1,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75961822, 0.66332816, 0.42293606, 0.74628054, 0.66337667,\n",
       "        0.51844784, 0.39549641, 0.37381742, 0.48381265, 0.41081774,\n",
       "        0.48419246, 0.47888069, 0.76984334, 0.70411132, 0.63604063,\n",
       "        0.49869362, 0.53156112, 0.54493951, 0.50739358, 0.53566634]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We see a small amount of ability from the model to predict sentiment.\n",
    "\n",
    "#### GRU\n",
    "\n",
    "We'll use the same format, albeit with different functions to build a GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Weights initialisation'''\n",
    "def weights_initialisation(batch_size,cell_size=50,x_size=2002):\n",
    "    #initialise the weights for the main 'estimator' node\n",
    "    weights={}\n",
    "    weights['gWCx'] = np.random.uniform(-1,1,[cell_size, x_size])\n",
    "    weights['gWCc'] = np.random.uniform(-1,1,[cell_size, cell_size])\n",
    "    weights['gWCb'] = np.random.uniform(-1,1,[cell_size,1])\n",
    "\n",
    "    #initialize weights for the 'update' node\n",
    "    weights['gWUx'] = np.random.uniform(-1,1,[cell_size, x_size])\n",
    "    weights['gWUc'] = np.random.uniform(-1,1,[cell_size, cell_size])\n",
    "    weights['gWUb'] = np.random.uniform(-1,1,[cell_size,1])\n",
    "\n",
    "    #initalise weights for output layer\n",
    "    weights['gWYa'] = np.random.uniform(-0.25,0.25,[1,cell_size])\n",
    "    weights['gWYb'] = np.random.uniform(-1,1,[1,1])\n",
    "    \n",
    "    a0 = np.zeros((cell_size, batch_size)) #create blank a0\n",
    "    return weights, a0\n",
    "\n",
    "def dict_initialisation_gru():\n",
    "    zc_dict, zu_dict, č_dict, cupd_dict, c_dict = {}, {}, {}, {}, {}\n",
    "    return zc_dict, zu_dict, č_dict, cupd_dict, c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gforward_prop_1(c_prev, weights, x, mask, t=0):\n",
    "    gWCx, gWCc, gWCb, gWUx, gWUc, gWUb, _, _ = weights.values()\n",
    "    \n",
    "    #create value for č by creating a z value to be tanhd + b\n",
    "    zc = gWCx @ x[:,t,:] + gWCc @ c_prev + gWCb\n",
    "    č = np.tanh(zc)\n",
    "    \n",
    "    #now create a value for c_update, and use mask to make this 0 if the mask is on.\n",
    "    zu = gWUx @ x[:,t,:] + gWUc @ c_prev + gWUb \n",
    "    Cupd = (1-mask[t]) * sigmoid(zu) #this will be zero if mask is 1\n",
    "    \n",
    "    #now update c\n",
    "    c = Cupd * č + (1-Cupd) * c_prev\n",
    "    \n",
    "    return zc, č, zu, Cupd, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcells_fw_prop(zc_dict, zu_dict, č_dict, cupd_dict, c_dict, a0, weights, x, mask,Tx):\n",
    "    \n",
    "    c_prev = a0 #to have an initial value to call the forward_prop function\n",
    "    for t in range(Tx):\n",
    "        zc, č, zu, Cupd, c = gforward_prop_1(c_prev=c_prev, t = t,x=x,mask = mask, weights=weights)\n",
    "        zc_dict[t+1], č_dict[t+1], zu_dict[t+1], cupd_dict[t+1], c_dict[t+1] = zc, č, zu, Cupd, c\n",
    "        c_prev = c\n",
    "    #we end by returning the dictionary for z and a values throughout the time period\n",
    "    return zc_dict, zu_dict, č_dict, cupd_dict, c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfull_fw_prop(a0, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask, Tx = Tx):\n",
    "    \n",
    "    #do the cell forward prop    \n",
    "    zc_dict, zu_dict, č_dict, cupd_dict, c_dict = gcells_fw_prop(zc_dict, zu_dict, č_dict, cupd_dict, c_dict, a0, weights, x, mask,Tx)\n",
    "\n",
    "    #extract last value and prediction\n",
    "    final_c = c_dict[Tx]\n",
    "    Yz = weights['gWYa'] @ final_c + weights['gWYb']\n",
    "    Ÿ = sigmoid(Yz)\n",
    "\n",
    "    return zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.02841736242788992\n",
      "Variance: 0.619418359769762\n"
     ]
    }
   ],
   "source": [
    "#when i initialise the weights, with a different x size, gWCx gives that value. same with cell size\n",
    "x_size_ut = 2002\n",
    "c_size_ut = 50\n",
    "batch_size= 1000\n",
    "x_ut, mask_x_ut = one_hot_x_40k[:,:,:batch_size], mask_x[:,:batch_size]\n",
    "\n",
    "weights_ut,a0= weights_initialisation(cell_size=c_size_ut,x_size=x_size_ut,batch_size=batch_size)\n",
    "assert weights_ut['gWUx'].shape == (c_size_ut, x_size_ut)\n",
    "\n",
    "#when i call the main function, c_dict[30] should be numbers with mean ~ 0 and var [0.25,1.25]\n",
    "zc_dict, zu_dict, č_dict, cupd_dict, c_dict = dict_initialisation_gru()\n",
    "zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ = gfull_fw_prop(a0, weights_ut, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x_ut, mask_x_ut, Tx)\n",
    "\n",
    "assert abs(np.mean(c_dict[30])) < 0.1\n",
    "assert 0.25 < np.var(c_dict[30]) < 1.25\n",
    "\n",
    "print(f'Mean: {np.mean(c_dict[30])}\\nVariance: {np.var(c_dict[30])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Alright now we're burning diesel.\n",
    "\n",
    "Time for back prop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gWCx', 'gWCc', 'gWCb', 'gWUx', 'gWUc', 'gWUb', 'gWYa', 'gWYb'])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_ut.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gback_prop_one_layer(prev_loss, t, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict,  x, mask_x,learning_rate = 0.01,batch_size=400):\n",
    "    \n",
    "    gWCx, gWCc, gWCb, gWUx, gWUc, gWUb, gWYa, gWYb = weights.values()\n",
    "    zc, zu, č, cupd, c = zc_dict[t], zu_dict[t], č_dict[t], cupd_dict[t], c_dict[t]\n",
    "    c_ = c_dict[t-1] #also take the last c value frm previous round for differentials\n",
    "    x_ = x[:,t-1,:] #relevant x values\n",
    "\n",
    "    dl_dc = prev_loss\n",
    "    '''first backpropogate through to get c previous'''\n",
    "    dc_dč = cupd\n",
    "    dc_dcupd = č - c_\n",
    "    dč_dzc = 1 - np.tanh(zc)**2\n",
    "    dcupd_dzu = sigmoid(zu) * (1-sigmoid(zu))\n",
    "    dzc_dc_1 = gWCc\n",
    "    dzu_dc_1 = gWCu\n",
    "    dc_dc_1 = (1-cupd) + (dc_dč * dč_dzc @ gWCc) + dc_dcupd * dcupd_dzu @ gWCu\n",
    "    \n",
    "#     #get dA and dZ as a base for the layer\n",
    "#     dL_dC = prev_loss #40000,50\n",
    "#     dA_dZ = ((1-mask_x[t-1]) * (1-(np.tanh(z_)**2))).T #40000,50\n",
    "    \n",
    "#     #get a previous\n",
    "#     dZ_dAp = WAa #50x50\n",
    "#     dA_dAp = mask_x[t-1].reshape(-1,1) + np.zeros_like((dL_dA)) + (dA_dZ @ dZ_dAp) #we create a mask of 1s everywhere here.\n",
    "#     dL_dAp = dL_dA * dA_dAp\n",
    "\n",
    "#     #differentiate with respect to weights\n",
    "#     dZ_dWAa = a__ #50 x m\n",
    "#     dL_dWAa = ((dL_dA * dA_dZ).T @ dZ_dWAa.T)/batch_size #weights dont matter if we are having a 0 on the mask\n",
    "\n",
    "#     dZ_dWAb = np.zeros((batch_size,1))+1\n",
    "#     dL_dWAb = ((dL_dA * dA_dZ).T @ dZ_dWAb)/batch_size\n",
    "\n",
    "#     dZ_dWAx = x_\n",
    "#     dL_dWAx = ((dL_dA * dA_dZ).T @ dZ_dWAx.T)/batch_size #weights dont matter if we are having a 0 on the mask\n",
    "    \n",
    "#     #now update the weights based on the findings here.\n",
    "#     return dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
