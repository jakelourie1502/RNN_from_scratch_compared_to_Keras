{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>polarity</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71042</th>\n",
       "      <td>b'Sat Jun 06 07:23:17 PDT 2009'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b\"I wasn't expecting it to grow, but it did! S...</td>\n",
       "      <td>b'wolfblaze'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41894</th>\n",
       "      <td>b'Sat May 30 09:24:42 PDT 2009'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'stressing out '</td>\n",
       "      <td>b'Jenne0187'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  date  polarity        query  \\\n",
       "71042  b'Sat Jun 06 07:23:17 PDT 2009'         4  b'NO_QUERY'   \n",
       "41894  b'Sat May 30 09:24:42 PDT 2009'         0  b'NO_QUERY'   \n",
       "\n",
       "                                                    text          user  \n",
       "71042  b\"I wasn't expecting it to grow, but it did! S...  b'wolfblaze'  \n",
       "41894                                  b'stressing out '  b'Jenne0187'  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_size = 100000\n",
    "ds = tfds.load('sentiment140', split='train', shuffle_files=True)\n",
    "df = tfds.as_dataframe(ds.take(df_size))\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    50021\n",
       "0    49979\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Note: Binary classification can be used. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bin_polarity'] = df['polarity'].apply(lambda x: 1 if x == 4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the RNN is that we take just the output of the final word (unless we're doing a bidirectional format, or a concatenated format where we concatenate the output of all the words)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating X values\n",
    "\n",
    "The text is in byte form, so we need to convert it to string form, and then use the split functionality to convert to a list. We ignore the first two characters which are a side effect of the conversion, and the last element in the string which is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns names: ['date', 'polarity', 'query', 'text', 'user', 'bin_polarity']\n"
     ]
    }
   ],
   "source": [
    "print(f'columns names: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separating the words into a list of words\n",
    "df['split_words'] = df['text'].apply(lambda x: str(x.lower())[2:].split()[:-1])\n",
    "df['split_words'] = df['split_words'].apply(lambda x: [x.translate(str.maketrans('', '', string.punctuation)) for x in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get the length of words\n",
    "# df['txt_length'] = df['split_words'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['txt_length'].value_counts(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Very few tweets are over 30 words, so we will create an RNN based on 30 words\n",
    "\n",
    "We now need to create a words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 30 #this is the value we use to specify how many words to consider in the tweet (first e.g. 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [ouch!, i, just, burned, my, finger, so, bad]\n",
       "1        [im, so, happy, i, found, some, one, who, want...\n",
       "2                       [@mceeyoshi, that's, my, movie, !]\n",
       "3                [my, sister, is, taking, over, my, room!]\n",
       "4                           [i, really, suck, at, pinball]\n",
       "                               ...                        \n",
       "99995    [i, want, the, sun, to, come, out!, why, does,...\n",
       "99996    [tuesday., another, infinite, crisis, at, the,...\n",
       "99997               [finally, my, picture, is, working...]\n",
       "99998                [it'll, all, be, ok.., til, thursday]\n",
       "99999    [@iphonetalktoday, have, an, ipod, touch, 32gb...\n",
       "Name: split_words, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['split_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create words dict'''\n",
    "\n",
    "no_words = 5000\n",
    "def create_words_dict(df,no_words):\n",
    "    \n",
    "    top_words = list(df['split_words'].explode().value_counts()[:no_words].index)\n",
    "    word_dict = {}\n",
    "    for idx, word in enumerate(top_words):\n",
    "        word_dict[idx+1] = word\n",
    "\n",
    "    #create additional values for 'word is over' and 'unknown'\n",
    "    word_dict[no_words+1] = 'word_over'\n",
    "    word_dict[0] = 'UNKNOWN-WORD'\n",
    "\n",
    "    #create reversed dict\n",
    "    word_dict_reversed = {}\n",
    "    for key, value in word_dict.items():\n",
    "        word_dict_reversed[value] = key\n",
    "    \n",
    "    return word_dict, word_dict_reversed\n",
    "word_dict_2k, word_dict_reversed = create_words_dict(df,no_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Our X values need to be vocab_size x 30 x m, one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the words in numeric index format\n",
    "    \n",
    "def create_x_array(df,Tx,no_words):\n",
    "    x = df['split_words'].apply(lambda x: np.array([word_dict_reversed[word] if word in word_dict_reversed.keys() else 0 for word in x]))\n",
    "\n",
    "    '''we're going to make all the x values the same length'''\n",
    "\n",
    "    array_x = np.zeros((df_size,Tx)) #blank array to put them in\n",
    "    for idx,arr in enumerate(x):\n",
    "        leng = len(arr) \n",
    "        #get length, and fork based on current size\n",
    "        if leng > Tx:\n",
    "            array_x[idx] = arr[:Tx] #just take first 30 values\n",
    "        elif leng < Tx:\n",
    "            array_x[idx] = np.append(arr,np.zeros(Tx-leng)+no_words+1) #append 30 minus the current length -1s\n",
    "    #put examples on the columns\n",
    "    array_x = array_x.T\n",
    "    return array_x,x\n",
    "\n",
    "\n",
    "array_x,x = create_x_array(df,Tx=30,no_words=no_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 100000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now one hot encode them'''\n",
    "def one_hot_process(array_x,Tx, batch_size):\n",
    "    one_hot_x_40k = np.zeros((no_words+2,Tx,batch_size))\n",
    "    for row_idx, row in enumerate(array_x[:,:batch_size]):\n",
    "        for exam_idx, word_val in enumerate(row):\n",
    "            one_hot_x_40k[int(word_val),row_idx,exam_idx] = 1\n",
    "    return one_hot_x_40k\n",
    "def mask_process(array_x, batch_size,no_words):\n",
    "    return np.where(array_x[:,:batch_size]==no_words+1,1,0)\n",
    "def one_hot_and_mask(array_x, Tx, batch_size,no_words):\n",
    "    one_hot_x_40k = one_hot_process(array_x,Tx, batch_size)\n",
    "    mask_x = mask_process(array_x, batch_size,no_words)\n",
    "    return one_hot_x_40k, mask_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_x_40k = one_hot_process(array_x,Tx, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "We've used 2002 in order to incorporatate the 2001th word (end of word) and unknown index. We can need to create a mask which tells the machine to skip if the mask is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_x  = mask_process(array_x, 10000,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 10000)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn Y into an array\n",
    "Y = np.array(df['bin_polarity']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "OK so we have our x inputs. Mask x, shaped 30 x 40000, and the actual onehotencoded x values, of shape 2001, 30, 40000\n",
    "\n",
    "In the basic RNN, ignoring the last stage for a second, each cell has three weights matrixes.\n",
    "\n",
    "### Forward Prop\n",
    "\n",
    "WAa - the weights applied to the previous cells outputs\n",
    "\n",
    "WAx - the weights applied to the X values\n",
    "\n",
    "WaB - a bias term. \n",
    "\n",
    "We have to make a choice of how large each cell is, and then initialise the weights. We'll use a Xavier initialization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a cell size of 50\n",
    "cell_size = 50\n",
    "x_size = no_words+no_words #Xavier_initialization of weights.\n",
    "WAa = np.random.uniform(-(6)/(np.sqrt(cell_size*2)),(6)/((cell_size*2)),[cell_size,cell_size]) \n",
    "WAb = np.random.uniform(-(6)/(np.sqrt(cell_size+1)),(6)/((cell_size+1)),[cell_size,1]) \n",
    "WAx = np.random.uniform(-(6)/(np.sqrt(cell_size+x_size)),(6)/((cell_size+x_size)),[cell_size,x_size]) \n",
    "\n",
    "#We need a weights matrix too for later\n",
    "WYa = np.random.uniform(-(6)/((cell_size+1)),(6)/((cell_size+1)),[1,cell_size])\n",
    "WYb = np.random.uniform(-(6)/((1+1)),(6)/((1+1)),[1,1])\n",
    "\n",
    "weights_dict = {}\n",
    "weights_dict['WAa'] = WAa\n",
    "weights_dict['WAb'] = WAb\n",
    "weights_dict['WAx'] = WAx\n",
    "weights_dict['WYa'] = WYa\n",
    "weights_dict['WYb'] = WYb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's implement one run of forward prop in order to replicate it going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these allow us to set some default values\n",
    "a0 = np.zeros((cell_size,df_size))\n",
    "z_dict, a_dict = {}, {}\n",
    "a_dict[0] = a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''See notes from above - this is just implemented as a function'''\n",
    "def forward_prop(a_prev,  WAa, WAx, WAb, x, mask,time_period=0):\n",
    "    \n",
    "    # #z1 - the pre-tanh values of a0, bias and x1 times weights\n",
    "    z1 = WAa @ a_prev + WAx @ x[:,time_period,:] + WAb\n",
    "    \n",
    "    # #a1 either equals a1 if mask ==1 or equal tanh z1 if mask == - \n",
    "    a1 = mask[time_period].reshape(1,-1) * a_prev + (1-mask[time_period]) * np.tanh(z1)\n",
    "    \n",
    "    return a1, z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The idea here is to loop over the time periods and run forward prop each time. We will need to re-use a1 and z1 when we come back and do the back prop, so we will store them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cells_fw_prop(Tx, z_dict, a_dict, a0,WAa, WAx, WAb, x, mask):\n",
    "    a_prev = a0 #to have an initial value to call the forward_prop function\n",
    "    for i in range(Tx):\n",
    "        a_, z_ = forward_prop(a_prev=a_prev, time_period = i,x=x,mask = mask,WAa=WAa, WAx=WAx, WAb=WAb)\n",
    "        z_dict[i+1], a_dict[i+1] = z_, a_\n",
    "        a_prev = a_\n",
    "    #we end by returning the dictionary for z and a values throughout the time period\n",
    "    return z_dict, a_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Lets put it all together in a forward prop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Yz):\n",
    "        return (1/(1+np.exp(-Yz)))\n",
    "    \n",
    "def full_fw_prop(weights_dict, z_dict, a_dict, x, mask, Tx = Tx):\n",
    "    \n",
    "    # initialize a0 and dict values\n",
    "    a0 = np.zeros((cell_size, x.shape[2]))\n",
    "    a_dict[0] = a0\n",
    "    WAa, WAb, WAx, WYa, WYb = weights_dict.values()\n",
    "\n",
    "    #do the cell forward prop    \n",
    "    z_dict, a_dict = cells_fw_prop(Tx = Tx, z_dict=z_dict, a_dict=a_dict, a0=a0,WAa=WAa, WAx=WAx, WAb=WAb, x=x,mask=mask)\n",
    "\n",
    "    #extract last value and prediction\n",
    "    final_a = a_dict[Tx]\n",
    "    Yz = WYa @ final_a + WYb\n",
    "   \n",
    "    Ÿ = sigmoid(Yz)\n",
    "\n",
    "    return z_dict, a_dict, Yz, Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Back Propogation\n",
    "\n",
    "We can conveniently use the differentiated sigmoid function, which gives\n",
    "\n",
    "DL/DZ = Ÿ - Y\n",
    "\n",
    "From there we can initially back calculate the values of the final portion, giving values for DL/DA, DL/DWYa, DL/DWYb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Turn Y into an array\n",
    "Y = np.array(df['bin_polarity']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start by initializing the backprop dicts\n",
    "dA_dict, dZ_dict = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "OK let's create the relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_one_layer(prev_Loss, t, weights_dict,  a_dict,z_dict,  X, mask_x,learning_rate = 0.01,batch_size=400):\n",
    "    \n",
    "    WAa, WAb, WAx, WYa, WYb = weights_dict.values()\n",
    "    a_ = a_dict[t]\n",
    "    z_ = z_dict[t]\n",
    "    a__ = a_dict[t-1]\n",
    "    x_ = X[:,t-1,:]\n",
    "\n",
    "    #get dA and dZ as a base for the layer\n",
    "    dL_dA = prev_Loss #40000,50\n",
    "    dA_dZa = ((1-mask_x[t-1]) * (1-(np.tanh(z_)**2))).T #40000,50\n",
    "    \n",
    "    #get a previous\n",
    "    dZ_dAp = WAa #50x50\n",
    "    dA_dAp = mask_x[t-1].reshape(-1,1) + np.zeros_like((dL_dA)) + (dA_dZa @ dZ_dAp) #we create a mask of 1s everywhere here.\n",
    "    dL_dAp = dL_dA * dA_dAp\n",
    "\n",
    "    #differentiate with respect to weights\n",
    "    dZ_dWAa = a__ #50 x m\n",
    "    dL_dWAa = ((dL_dA * dA_dZa).T @ dZ_dWAa.T)/batch_size #weights dont matter if we are having a 0 on the mask\n",
    "\n",
    "    dZ_dWAb = np.zeros((batch_size,1))+1\n",
    "    dL_dWAb = ((dL_dA * dA_dZa).T @ dZ_dWAb)/batch_size\n",
    "\n",
    "    dZ_dWAx = x_\n",
    "    dL_dWAx = ((dL_dA * dA_dZa).T @ dZ_dWAx.T)/batch_size #weights dont matter if we are having a 0 on the mask\n",
    "    \n",
    "    #now update the weights based on the findings here.\n",
    "    return dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_full_swing(Ÿ, Y, Tx, weights_dict,  a_dict,z_dict,  X, mask_x,learning_rate = 0.01,batch_size = 100):\n",
    "    \n",
    "    '''initial loss function w.r.t Z'''\n",
    "    WAa, WAb, WAx, WYa, WYb = weights_dict.values()\n",
    "    dL_dZ = (Ÿ - Y).T #40000 x 1\n",
    "\n",
    "\n",
    "    # first order diffs\n",
    "    dZ_dA = WYa # 1 x 50\n",
    "    dZ_dWYa = a_dict[Tx] # 50 x 40000\n",
    "    dZ_dB = np.zeros_like(dL_dZ) + 1\n",
    "\n",
    "    #chain ruled diffs\n",
    "    dL_dA = dL_dZ @ dZ_dA\n",
    "    dL_dWYa = (dZ_dWYa @ dL_dZ).T / batch_size\n",
    "    dL_dWYb = (dZ_dB.T @ dL_dZ)/ batch_size\n",
    "    \n",
    "    weights_dict['WYa'] -= learning_rate * dL_dWYa\n",
    "    weights_dict['WYb'] -= learning_rate * dL_dWYb\n",
    "    \n",
    "    prev_loss = dL_dA\n",
    "    '''now go through the other functions'''\n",
    "    for t in reversed(range(1,Tx)):\n",
    "        #extract relevant differentials for updating backprop and also carrying on the backprop through the layers\n",
    "        dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp = back_prop_one_layer(prev_Loss=prev_loss, t=t, weights_dict = weights_dict,X=x,  a_dict=a_dict,z_dict=z_dict,mask_x=mask_x,batch_size=batch_size)\n",
    "        weights_dict['WAa'] -= learning_rate*dL_dWAa\n",
    "        weights_dict['WAx'] -= learning_rate*dL_dWAx\n",
    "        weights_dict['WAb'] -= learning_rate*dL_dWAb\n",
    "        prev_loss = dL_dAp\n",
    "        return dL_dZ, dZ_dA, dZ_dB, dL_dA, dL_dWYa, dL_dWYb, dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back_prop_full_swing(Ÿ=Ÿ, Y=Y, Tx=Tx, weights_dict=weights_dict,  a_dict=a_dict,z_dict=z_dict,  X=one_hot_x_40k, mask_x=mask_x,learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Putting it all together\n",
    "\n",
    "Let's now create a function which runs forward prop and backward prop in batches, and calculates the loss at each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.48\n",
      "Epoch: 0, loss: 0.7342162588514232\n",
      "Accuracy score: 0.5299009900990099\n",
      "Epoch: 200, loss: 0.68143362146241\n",
      "Accuracy score: 0.5348514851485149\n",
      "Epoch: 400, loss: 0.6875637372372094\n"
     ]
    }
   ],
   "source": [
    "'''set some initial parameters'''\n",
    "epochs = 500\n",
    "cell_size = 64\n",
    "x_size = no_words+2\n",
    "batches=1000\n",
    "batch_size = int(df_size//batches) #so 400\n",
    "loss_reporting_frequency = 200\n",
    "\n",
    "''' initialize weights'''\n",
    "def RNN_init(cell_size, x_size):\n",
    "    WAx = np.random.uniform(-1,1,[cell_size,x_size])\n",
    "    WAb = np.random.uniform(-1,1,[cell_size,1])\n",
    "    WAa = np.random.uniform(-0.1,0.1,[cell_size,cell_size])\n",
    "    WYa = np.random.uniform(-0.25,0.25,[1,cell_size])\n",
    "    WYb = np.random.uniform(-1,1,[1,1])\n",
    "    weights_dict = {}\n",
    "    weights_dict['WAa'] = WAa\n",
    "    weights_dict['WAb'] = WAb\n",
    "    weights_dict['WAx'] = WAx\n",
    "    weights_dict['WYa'] = WYa\n",
    "    weights_dict['WYb'] = WYb\n",
    "    z_dict, a_dict= {}, {}\n",
    "    losses, accs,  ys, y_hats = [],[],[],[]\n",
    "    return WAx, WAb, WAa, WYa, WYb, weights_dict, z_dict, a_dict, losses, accs, ys, y_hats\n",
    "\n",
    "def batching_rnn(epoch,batches, batch_size):\n",
    "    round_ = epoch % batches #deals with if it's the first of a new set of batches\n",
    "    n, k = batch_size*round_, batch_size*(1 + round_)\n",
    "    return n,k \n",
    "\n",
    "def get_loss_metrics_rnn(y,Ÿ,batch_size, epoch,k):\n",
    "    loss = -np.sum(y*np.log(Ÿ) + (1-y)*np.log(1-Ÿ))/batch_size\n",
    "    ys.append(y)\n",
    "    y_hats.append(Ÿ)\n",
    "    accuracy = np.sum(y==np.where(Ÿ>0.5,1,0))/batch_size\n",
    "    losses.append(loss)\n",
    "    accs.append(accuracy)\n",
    "    if epoch % k == 0:\n",
    "        print(f'Accuracy score: {np.mean(accs[(epoch-batch_size):])}')    \n",
    "        print(f'Epoch: {epoch}, loss: {loss}')\n",
    "\n",
    "WAx, WAb, WAa, WYa, WYb, weights_dict, z_dict, a_dict, losses, accs, ys, y_hats = RNN_init(cell_size,x_size)        \n",
    "for epoch in range(epochs):\n",
    "    n, k = batching_rnn(epoch,batches,batch_size) #create batching indexes\n",
    "\n",
    "    #get specific x and y values\n",
    "    x, mask_x_ = one_hot_and_mask(array_x[:,n:k],Tx,batch_size,no_words=no_words)\n",
    "    y = Y[0,n:k]\n",
    "    \n",
    "    #run fw prop\n",
    "    z_dict, a_dict, Yz, Ÿ = full_fw_prop(Tx = Tx, z_dict=z_dict, a_dict=a_dict, weights_dict = weights_dict, x=x,mask=mask_x_)\n",
    "    \n",
    "    #append metrics and print loss if relevant\n",
    "    get_loss_metrics_rnn(y,Ÿ, batch_size, epoch, loss_reporting_frequency)\n",
    "    \n",
    "    #run backprop\n",
    "    dL_dZ, dZ_dA, dZ_dB, dL_dA, dL_dWYa, dL_dWYb, dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp = back_prop_full_swing(Ÿ=Ÿ, Y=y, Tx=Tx, weights_dict=weights_dict,  a_dict=a_dict,z_dict=z_dict,  X=x, mask_x=mask_x_,learning_rate = 0.1,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12135689, -0.71660081,  0.23251359, ...,  0.17323553,\n",
       "        -0.87838207,  0.37504349],\n",
       "       [-0.88134366, -0.90306568, -0.94579807, ..., -0.67807376,\n",
       "         0.02865951,  0.25115641],\n",
       "       [-0.74451552, -0.46015606,  0.47691773, ..., -0.19210396,\n",
       "        -0.75220213,  0.31057394],\n",
       "       ...,\n",
       "       [-0.31304665,  0.00483609, -0.70442376, ...,  0.49018579,\n",
       "         0.8468961 ,  0.81864161],\n",
       "       [ 0.63358706, -0.85337845,  0.63464234, ..., -0.21338884,\n",
       "        -0.62683255, -0.18111244],\n",
       "       [ 0.63527926,  0.6289208 ,  0.96180813, ..., -0.10453993,\n",
       "        -0.0856621 ,  0.40286678]])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dict[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We see a small amount of ability from the model to predict sentiment.\n",
    "\n",
    "#### GRU\n",
    "\n",
    "We'll use the same format, albeit with different functions to build a GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Weights initialisation'''\n",
    "def weights_initialisation(batch_size,cell_size=50,x_size=2002):\n",
    "    #initialise the weights for the main 'estimator' node\n",
    "    weights={}\n",
    "    weights['gWCx'] = np.random.uniform(-1,1,[cell_size, x_size])\n",
    "    weights['gWCc'] = np.random.uniform(-1,1,[cell_size, cell_size])\n",
    "    weights['gWCb'] = np.random.uniform(-1,1,[cell_size,1])\n",
    "\n",
    "    #initialize weights for the 'update' node\n",
    "    weights['gWUx'] = np.random.uniform(-1,1,[cell_size, x_size])\n",
    "    weights['gWUc'] = np.random.uniform(-1,1,[cell_size, cell_size])\n",
    "    weights['gWUb'] = np.random.uniform(-1,1,[cell_size,1])\n",
    "\n",
    "    #initalise weights for output layer\n",
    "    weights['gWYa'] = np.random.uniform(-0.25,0.25,[1,cell_size])\n",
    "    weights['gWYb'] = np.random.uniform(-1,1,[1,1])\n",
    "    \n",
    "    a0 = np.zeros((cell_size, batch_size)) #create blank a0\n",
    "    return weights, a0\n",
    "\n",
    "def dict_initialisation_gru():\n",
    "    zc_dict, zu_dict, č_dict, cupd_dict, c_dict = {}, {}, {}, {}, {}\n",
    "    return zc_dict, zu_dict, č_dict, cupd_dict, c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gforward_prop_1(c_prev, weights, x, mask, t=0):\n",
    "    gWCx, gWCc, gWCb, gWUx, gWUc, gWUb, _, _ = weights.values()\n",
    "    \n",
    "    #create value for č by creating a z value to be tanhd + b\n",
    "    \n",
    "    zc = gWCx @ x[:,t,:] + gWCc @ c_prev + gWCb\n",
    "    č = np.tanh(zc)\n",
    "    \n",
    "    #now create a value for c_update, and use mask to make this 0 if the mask is on.\n",
    "    zu = gWUx @ x[:,t,:] + gWUc @ c_prev + gWUb \n",
    "    Cupd = (1-mask[t]) * sigmoid(zu) #this will be zero if mask is 1\n",
    "    \n",
    "    #now update c\n",
    "    c = Cupd * č + (1-Cupd) * c_prev\n",
    "    \n",
    "    return zc, č, zu, Cupd, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcells_fw_prop(zc_dict, zu_dict, č_dict, cupd_dict, c_dict, a0, weights, x, mask,Tx):\n",
    "    \n",
    "    c_prev = a0 #to have an initial value to call the forward_prop function\n",
    "    for t in range(Tx):\n",
    "        zc, č, zu, Cupd, c = gforward_prop_1(c_prev=c_prev, t = t,x=x,mask = mask, weights=weights)\n",
    "        zc_dict[t+1], č_dict[t+1], zu_dict[t+1], cupd_dict[t+1], c_dict[t+1] = zc, č, zu, Cupd, c\n",
    "        c_prev = c\n",
    "    #we end by returning the dictionary for z and a values throughout the time period\n",
    "    return zc_dict, zu_dict, č_dict, cupd_dict, c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfull_fw_prop(a0, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask, Tx = Tx):\n",
    "    \n",
    "    #do the cell forward prop    \n",
    "    zc_dict, zu_dict, č_dict, cupd_dict, c_dict = gcells_fw_prop(zc_dict, zu_dict, č_dict, cupd_dict, c_dict, a0, weights, x, mask,Tx)\n",
    "\n",
    "    #extract last value and prediction\n",
    "    final_c = c_dict[Tx]\n",
    "    Yz = weights['gWYa'] @ final_c + weights['gWYb']\n",
    "    Ÿ = sigmoid(Yz)\n",
    "\n",
    "    return zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Yz):\n",
    "        return (1/(1+np.exp(-Yz)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002, 30, 10000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_x_40k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.010948738591262427\n",
      "Variance: 0.7115851806965051\n"
     ]
    }
   ],
   "source": [
    "#when i initialise the weights, with a different x size, gWCx gives that value. same with cell size\n",
    "x_size_ut = 5002\n",
    "c_size_ut = 50\n",
    "batch_size= 1000\n",
    "x_ut, mask_x_ut = one_hot_x_40k[:,:,:batch_size], mask_x[:,:batch_size]\n",
    "y_ut = Y[:,:batch_size]\n",
    "\n",
    "weights_ut,a0= weights_initialisation(cell_size=c_size_ut,x_size=x_size_ut,batch_size=batch_size)\n",
    "assert weights_ut['gWUx'].shape == (c_size_ut, x_size_ut)\n",
    "\n",
    "#when i call the main function, c_dict[30] should be numbers with mean ~ 0 and var [0.25,1.25]\n",
    "zc_dict, zu_dict, č_dict, cupd_dict, c_dict = dict_initialisation_gru()\n",
    "zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ = gfull_fw_prop(a0, weights_ut, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x_ut, mask_x_ut, Tx)\n",
    "\n",
    "assert abs(np.mean(c_dict[30])) < 0.1\n",
    "assert 0.25 < np.var(c_dict[30]) < 1.25\n",
    "\n",
    "print(f'Mean: {np.mean(c_dict[30])}\\nVariance: {np.var(c_dict[30])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Alright now we're burning diesel.\n",
    "\n",
    "Time for back prop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gback_prop_one_layer(prev_loss, t, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict,  x, mask_x,learning_rate = 0.01,batch_size=400):\n",
    "    \n",
    "    gWCx, gWCc, gWCb, gWUx, gWUc, gWUb, gWYa, gWYb = weights.values()\n",
    "    zc, zu, č, cupd, c = zc_dict[t], zu_dict[t], č_dict[t], cupd_dict[t], c_dict[t]\n",
    "    c_ = c_dict[t-1] #also take the last c value frm previous round for differentials\n",
    "    x_ = x[:,t-1,:] #relevant x values, 2000 x m\n",
    "\n",
    "    dl_dc = prev_loss #m,50\n",
    "\n",
    "    '''first backpropogate through to get c previous'''\n",
    "    dc_dč = cupd #50 x m \n",
    "    dc_dcupd = č - c_ #50 x m\n",
    "    dč_dzc = 1 - np.tanh(zc)**2 #50 x m\n",
    "    dcupd_dzu = sigmoid(zu) * (1-sigmoid(zu)) #50 x m\n",
    "    dzc_dc_1 = gWCc #50,50\n",
    "    dzu_dc_1 = gWUc #50,50\n",
    "\n",
    "    #1- cupd is 50, m\n",
    "    dc_dc_1 = (1-cupd).T + ((dc_dč * dč_dzc).T @ gWCc) + ((dc_dcupd * dcupd_dzu).T @ gWUc) #ends as 50 x m \n",
    "    dl_dc_1 = dl_dc * dc_dc_1 #m x 50\n",
    "\n",
    "    \n",
    "    '''now get the weights with respect to L'''\n",
    "    #first three terms are element wise because they are the ru term and the sigmoid term.\n",
    "\n",
    "    dL_dWCx = (dl_dc.T * dc_dč * dč_dzc) @ x_.T #50 x m, m x 2000 = 50 x 2000 (same shape as WCx)\n",
    "    dL_dWCb = ((dl_dc.T * dc_dč * dč_dzc) @ (np.zeros((batch_size,1))+1))/batch_size #50 x m, m x 1 = 50 x1\n",
    "    dL_dWCc = (dl_dc.T * dc_dč * dč_dzc) @ c_.T #50 x m, m x 50 = 50 x 50\n",
    "    dL_dWUx = (dl_dc.T * dc_dcupd * dcupd_dzu) @ x_.T\n",
    "    dL_dWUb = ((dl_dc.T * dc_dcupd * dcupd_dzu) @ (np.zeros((batch_size,1))+1))/batch_size #50 x m, m x 1 = 50 x1\n",
    "    dL_dWUc = (dl_dc.T * dc_dcupd * dcupd_dzu) @ c_.T #50 x m, m x 50 = 50 x 50\n",
    "    return dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002, 30, 1000)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gback_prop_full_swing(Ÿ, Y, Tx, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask_x,learning_rate = 0.01,batch_size = 100):\n",
    "    \n",
    "    '''initial loss function w.r.t Z'''\n",
    "    gWCx, gWCc, gWCb, gWUx, gWUc, gWUb, gWYa, gWYb = weights.values()\n",
    "    dL_dZy = (Ÿ - Y).T #40000 x 1\n",
    "\n",
    "    # first order diffs\n",
    "    dZ_dC = gWYa # 1 x 50\n",
    "    dZ_dWYa = c_dict[Tx] # 50 x 40000\n",
    "    dZ_dB = np.zeros_like(dL_dZy) + 1\n",
    "\n",
    "    #chain ruled diffs\n",
    "    dL_dC = dL_dZy @ dZ_dC\n",
    "    dL_dWYa = (dZ_dWYa @ dL_dZy).T / batch_size\n",
    "    dL_dWYb = (dZ_dB.T @ dL_dZy)/ batch_size\n",
    "    \n",
    "    #update end weights.\n",
    "    weights['gWYa'] -= learning_rate * dL_dWYa\n",
    "    weights['gWYb'] -= learning_rate * dL_dWYb\n",
    "    \n",
    "    prev_loss = dL_dC\n",
    "    '''now go through the other functions'''\n",
    "    for t in reversed(range(int(Tx))):\n",
    "        #extract relevant differentials for updating backprop and also carrying on the backprop through the layers\n",
    "        dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1 = gback_prop_one_layer(prev_loss, t, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict,  x, mask_x,learning_rate = 0.01,batch_size=batch_size)\n",
    "        weights['gWCx'] -= learning_rate*dL_dWCx\n",
    "        weights['gWCc'] -= learning_rate*dL_dWCc\n",
    "        weights['gWCb'] -= learning_rate*dL_dWCb\n",
    "        weights['gWUx'] -= learning_rate*dL_dWUx\n",
    "        weights['gWUc'] -= learning_rate*dL_dWUc\n",
    "        weights['gWCb'] -= learning_rate*dL_dWUb\n",
    "        prev_loss = dl_dc_1\n",
    "        return dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1, dL_dWYa,dL_dWYb, dL_dZy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Unit tests\n",
    "\n",
    "Manual: check that the top 4 values are 10%+ different versus the bottom 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518.3371162347032 0.39880540574922674\n",
      "0.3367040053181182 0.033920416628132805\n",
      "447.87188518370573 0.348103681954843\n",
      "0.3541276330397732 0.09629933341112924\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(weights_ut['gWCx']),np.sum(weights_ut['gWYa']))\n",
    "print(np.var(weights_ut['gWCx']),np.var(weights_ut['gWYa']))\n",
    "for i in range(10):\n",
    "    dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1, dL_dWYa,dL_dWYb, dL_dZy = gback_prop_full_swing(Ÿ, y_ut, Tx, weights_ut, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x_ut, mask_x_ut,learning_rate = 0.1,batch_size = 1000)\n",
    "print(np.sum(weights_ut['gWCx']),np.sum(weights_ut['gWYa']))\n",
    "print(np.var(weights_ut['gWCx']),np.var(weights_ut['gWYa']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Putting it all together with the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set epochs, cell_size, x_size (from no_words)\n",
    "epochs = 50\n",
    "cell_size= 300\n",
    "voc_siz = no_words+2\n",
    "batch_size = 250\n",
    "lr = 0.05\n",
    "\n",
    "def gru_train(epochs, cell_size, voc_size,batch_size,no_words,lr, Y, array_x, Tx, reporting_freq = 200, arr_or_df = 'array',normed_emb=False):\n",
    "    # initialize weights\n",
    "    weights,a0= weights_initialisation(cell_size=cell_size,x_size=voc_size,batch_size=batch_size)\n",
    "    zc_dict, zu_dict, č_dict, cupd_dict, c_dict = dict_initialisation_gru()\n",
    "\n",
    "    #create loss & accuracy lists\n",
    "    glosses = []\n",
    "    gaccs = []\n",
    "\n",
    "    #create batches\n",
    "    batches = int(Y.shape[1]//batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        #create batch of x, x mask and y\n",
    "        n, k = batching_rnn(epoch, batches, batch_size)\n",
    "        #allow branching for the word embedding funciton which comes in df form\n",
    "        if arr_or_df == 'array':\n",
    "            x, mask_x_ = one_hot_and_mask(array_x[:,n:k],Tx,batch_size,no_words=no_words)\n",
    "        elif arr_or_df == 'df':\n",
    "            x, mask_x_ = create_embed_batch(df,n,k,Tx,norm=normed_emb)\n",
    "        y = Y[0,n:k]\n",
    "        \n",
    "        #forward prop and loss calculation\n",
    "        zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ = gfull_fw_prop(a0, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask_x_, Tx)\n",
    "        \n",
    "        '''create loss metrics for reporting'''\n",
    "        loss = -np.sum(y*np.log(Ÿ) + (1-y)*np.log(1-Ÿ))/batch_size\n",
    "        accuracy = np.sum(y==np.where(Ÿ>0.5,1,0))/batch_size\n",
    "        glosses.append(loss); gaccs.append(accuracy)\n",
    "        if epoch % reporting_freq == 0:\n",
    "            print(f'Accuracy score: {np.mean(gaccs[(epoch-batch_size):])}')    \n",
    "            print(f'Epoch: {epoch}, loss: {loss}')\n",
    "\n",
    "        #back prop\n",
    "        dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1, dL_dWYa,dL_dWYb, dL_dZy = gback_prop_full_swing(Ÿ, y, Tx, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask_x,learning_rate = lr,batch_size = batch_size)\n",
    "    \n",
    "    return weights, glosses, gaccs, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ, dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1, dL_dWYa,dL_dWYb, dL_dZy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.504\n",
      "Epoch: 0, loss: 1.0413344157199753\n",
      "Accuracy score: 0.5236015936254982\n",
      "Epoch: 300, loss: 0.6973130632719766\n",
      "Accuracy score: 0.5559362549800797\n",
      "Epoch: 600, loss: 0.6762625697859393\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-b8ce71f13d40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuple_of_outcomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc_siz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mno_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreporting_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marr_or_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-201-1855bc189d91>\u001b[0m in \u001b[0;36mgru_train\u001b[0;34m(epochs, cell_size, voc_size, batch_size, no_words, lr, Y, array_x, Tx, reporting_freq, arr_or_df)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#forward prop and loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mzc_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzu_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mč_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mŸ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgfull_fw_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzc_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzu_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mč_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_x_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m'''create loss metrics for reporting'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-191-dbc853455fc2>\u001b[0m in \u001b[0;36mgfull_fw_prop\u001b[0;34m(a0, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask, Tx)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#do the cell forward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mzc_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzu_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mč_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcells_fw_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzc_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzu_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mč_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#extract last value and prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-190-766e95e7bf87>\u001b[0m in \u001b[0;36mgcells_fw_prop\u001b[0;34m(zc_dict, zu_dict, č_dict, cupd_dict, c_dict, a0, weights, x, mask, Tx)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mc_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma0\u001b[0m \u001b[0;31m#to have an initial value to call the forward_prop function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mzc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mč\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCupd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgforward_prop_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_prev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mzc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mč_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzu_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupd_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mč\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCupd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mc_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-6c8e79507968>\u001b[0m in \u001b[0;36mgforward_prop_1\u001b[0;34m(c_prev, weights, x, mask, t)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#now create a value for c_update, and use mask to make this 0 if the mask is on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mzu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgWUx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgWUc\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mc_prev\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgWUb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mCupd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this will be zero if mask is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuple_of_outcomes = gru_train(1000, cell_size, voc_siz,batch_size,no_words,lr, Y, array_x, Tx,reporting_freq=300,arr_or_df='array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdb2dcb2e50>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(glosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>polarity</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>bin_polarity</th>\n",
       "      <th>split_words</th>\n",
       "      <th>txt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189562</th>\n",
       "      <td>b'Wed Jun 03 02:05:40 PDT 2009'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'@Shikha_M yeah do...but its hacked '</td>\n",
       "      <td>b'Pro_94JBIT'</td>\n",
       "      <td>0</td>\n",
       "      <td>[@Shikha_M, yeah, do...but, its, hacked]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75941</th>\n",
       "      <td>b'Mon Jun 15 15:14:00 PDT 2009'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'@radadams Do want '</td>\n",
       "      <td>b'Mayeh'</td>\n",
       "      <td>0</td>\n",
       "      <td>[@radadams, Do, want]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88864</th>\n",
       "      <td>b'Sun Jun 21 01:14:59 PDT 2009'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b\"I don't get it at all \"</td>\n",
       "      <td>b'jodeeluv'</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, don't, get, it, at, all]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>b'Fri May 29 12:41:21 PDT 2009'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'@PixyStixKitten The Last Unicorn...one of my...</td>\n",
       "      <td>b'rangerlyn'</td>\n",
       "      <td>1</td>\n",
       "      <td>[@PixyStixKitten, The, Last, Unicorn...one, of...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157169</th>\n",
       "      <td>b'Sat Jun 06 22:57:06 PDT 2009'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'listening to anberlin and passsing out '</td>\n",
       "      <td>b'ticklemexmik'</td>\n",
       "      <td>1</td>\n",
       "      <td>[listening, to, anberlin, and, passsing, out]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   date  polarity        query  \\\n",
       "189562  b'Wed Jun 03 02:05:40 PDT 2009'         0  b'NO_QUERY'   \n",
       "75941   b'Mon Jun 15 15:14:00 PDT 2009'         0  b'NO_QUERY'   \n",
       "88864   b'Sun Jun 21 01:14:59 PDT 2009'         0  b'NO_QUERY'   \n",
       "39095   b'Fri May 29 12:41:21 PDT 2009'         4  b'NO_QUERY'   \n",
       "157169  b'Sat Jun 06 22:57:06 PDT 2009'         4  b'NO_QUERY'   \n",
       "\n",
       "                                                     text             user  \\\n",
       "189562             b'@Shikha_M yeah do...but its hacked '    b'Pro_94JBIT'   \n",
       "75941                               b'@radadams Do want '         b'Mayeh'   \n",
       "88864                           b\"I don't get it at all \"      b'jodeeluv'   \n",
       "39095   b'@PixyStixKitten The Last Unicorn...one of my...     b'rangerlyn'   \n",
       "157169         b'listening to anberlin and passsing out '  b'ticklemexmik'   \n",
       "\n",
       "        bin_polarity                                        split_words  \\\n",
       "189562             0           [@Shikha_M, yeah, do...but, its, hacked]   \n",
       "75941              0                              [@radadams, Do, want]   \n",
       "88864              0                       [I, don't, get, it, at, all]   \n",
       "39095              1  [@PixyStixKitten, The, Last, Unicorn...one, of...   \n",
       "157169             1      [listening, to, anberlin, and, passsing, out]   \n",
       "\n",
       "        txt_length  \n",
       "189562           5  \n",
       "75941            3  \n",
       "88864            6  \n",
       "39095            7  \n",
       "157169           6  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Let's try word embedding to see if this improves\n",
    "\n",
    "We can use pre-made word embedding vectors from glove, which has 400,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/users/jacoblourie/RNN_games/glove.6B.50d.txt'\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_index, index_to_words, word_to_vec_map = read_glove_vecs(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "We want to pass a list of words to a function, which maps them to the word embedding vectors, pads those vectors out and passes them to a RNN\n",
    "\n",
    "let's look an example list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = df.loc[1]['split_words']\n",
    "#create a blank vector with 50 dims for the embedding dims, 30 time series slots, and 1 example\n",
    "t_vs = 30\n",
    "m_vs = 1\n",
    "def create_embedding_vecs(wordlist, t, m,norm):\n",
    "    x_embed_vec = np.zeros((50,t)) \n",
    "    x_embed_mask = np.zeros((t,)) + 1\n",
    "\n",
    "    for word_idx in range(t):\n",
    "        #check the word exists\n",
    "        if (word_idx < len(wordlist)):\n",
    "            if wordlist[word_idx] in words_to_index.keys():\n",
    "                x_embed_vec[:,word_idx] = word_to_vec_map[wordlist[word_idx]]\n",
    "                if norm==True:\n",
    "                    x_embed_vec[:,word_idx] = x_embed_vec[:,word_idx] / np.sum(x_embed_vec[:,word_idx])\n",
    "                x_embed_mask[word_idx] = 0\n",
    "\n",
    "    return x_embed_vec, x_embed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample_vec, x_sample_mask = create_embedding_vecs(sample_list, t_vs, m_vs,norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embed_batch(df,n,k,t, norm=True):\n",
    "    m = k-n #examples is last index minus firsti ndex\n",
    "    x_sample_multi = df.loc[n:k]['split_words']\n",
    "    x_array_emb = np.zeros((50,t, m))\n",
    "    x_array_emb_mask = np.zeros((t,m))\n",
    "    for i in range(m):\n",
    "        x_array_emb[:,:,i], x_array_emb_mask[:,i] = create_embedding_vecs(x_sample_multi.iloc[i],t,1,norm)\n",
    "    return x_array_emb, x_array_emb_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array_emb_s, x_array_emb_mask = create_embed_batch(df,300,310,t_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can now put this into our RNN model using the functions defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 30, 290)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_array_emb_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''set some initial parameters'''\n",
    "epochs = 5000\n",
    "cell_size = 64\n",
    "x_size = 50\n",
    "batches=1000\n",
    "\n",
    "loss_reporting_frequency = 500\n",
    "Tx = 30\n",
    "train_size = 90000\n",
    "df_train = df.loc[:train_size-1,:]\n",
    "Y_train = Y[:train_size]\n",
    "df_val = df.loc[train_size:,:]\n",
    "Y_train = Y[:,:train_size]\n",
    "Y_val = Y[:,train_size:]\n",
    "batch_size = int(train_size//batches) #so 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.5888888888888889\n",
      "Epoch: 0, loss: 0.7665295146433962\n",
      "Accuracy score: 0.5482295482295482\n",
      "Epoch: 500, loss: 0.7093246022770234\n",
      "Accuracy score: 0.5494505494505494\n",
      "Epoch: 1000, loss: 0.7032911199154587\n",
      "Accuracy score: 0.5548229548229549\n",
      "Epoch: 1500, loss: 0.7114733799116136\n",
      "Accuracy score: 0.5526251526251525\n",
      "Epoch: 2000, loss: 0.705931443217046\n",
      "Accuracy score: 0.5564102564102564\n",
      "Epoch: 2500, loss: 0.7105278292142672\n",
      "Accuracy score: 0.550915750915751\n",
      "Epoch: 3000, loss: 0.7104358544807152\n",
      "Accuracy score: 0.5567765567765568\n",
      "Epoch: 3500, loss: 0.7095163910444275\n",
      "Accuracy score: 0.5487179487179487\n",
      "Epoch: 4000, loss: 0.7139658722160506\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-b2eb7872ca00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#run fw prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mz_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mŸ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_fw_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_x_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#append metrics and print loss if relevant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-497f134ddcdd>\u001b[0m in \u001b[0;36mfull_fw_prop\u001b[0;34m(weights_dict, z_dict, a_dict, x, mask, Tx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#do the cell forward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mz_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcells_fw_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWAa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWAa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWAx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWAx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWAb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWAb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#extract last value and prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-d507994dc516>\u001b[0m in \u001b[0;36mcells_fw_prop\u001b[0;34m(Tx, z_dict, a_dict, a0, WAa, WAx, WAb, x, mask)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ma_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma0\u001b[0m \u001b[0;31m#to have an initial value to call the forward_prop function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0ma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_prev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWAa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWAa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWAx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWAx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWAb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWAb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mz_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0ma_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-176cc41e35af>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(a_prev, WAa, WAx, WAb, x, mask, time_period)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# #a1 either equals a1 if mask ==1 or equal tanh z1 if mask == -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_period\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma_prev\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_period\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "WAx, WAb, WAa, WYa, WYb, weights_dict, z_dict, a_dict, losses, accs, ys, y_hats = RNN_init(cell_size,x_size)        \n",
    "for epoch in range(epochs):\n",
    "    n, k = batching_rnn(epoch,batches,batch_size) #create batching indexes\n",
    "\n",
    "    #get specific x, x_mask and y values\n",
    "    x, mask_x_ = create_embed_batch(df_train,n,k,Tx)\n",
    "    #normalize x\n",
    "    y = Y_train[0,n:k]\n",
    "    \n",
    "    #run fw prop\n",
    "    z_dict, a_dict, Yz, Ÿ = full_fw_prop(Tx = Tx, z_dict=z_dict, a_dict=a_dict, weights_dict = weights_dict, x=x,mask=mask_x_)\n",
    "    \n",
    "    #append metrics and print loss if relevant\n",
    "    get_loss_metrics_rnn(y,Ÿ, batch_size, epoch, loss_reporting_frequency)\n",
    "    \n",
    "    #run backprop\n",
    "    dL_dZ, dZ_dA, dZ_dB, dL_dA, dL_dWYa, dL_dWYb, dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp = back_prop_full_swing(Ÿ=Ÿ, Y=y, Tx=Tx, weights_dict=weights_dict,  a_dict=a_dict,z_dict=z_dict,  X=x, mask_x=mask_x_,learning_rate = 0.1,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Definite improvement. let's try it once in the GRU before seeing if keras gives significantly better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.51\n",
      "Epoch: 0, loss: 0.9784587262714\n",
      "Accuracy score: 0.5145544554455445\n",
      "Epoch: 300, loss: 0.7942555523586107\n",
      "Accuracy score: 0.521980198019802\n",
      "Epoch: 600, loss: 0.776554894255418\n",
      "Accuracy score: 0.5374257425742575\n",
      "Epoch: 900, loss: 0.7577297310385734\n",
      "Accuracy score: 0.5341584158415841\n",
      "Epoch: 1200, loss: 0.7466458172638891\n",
      "Accuracy score: 0.548910891089109\n",
      "Epoch: 1500, loss: 0.7165238863880101\n",
      "Accuracy score: 0.5543564356435644\n",
      "Epoch: 1800, loss: 0.6742760376924564\n",
      "Accuracy score: 0.5608910891089111\n",
      "Epoch: 2100, loss: 0.705647581754989\n",
      "Accuracy score: 0.5658415841584158\n",
      "Epoch: 2400, loss: 0.6839740333863059\n",
      "Accuracy score: 0.5662376237623763\n",
      "Epoch: 2700, loss: 0.6956065426850402\n",
      "Accuracy score: 0.5757425742574258\n",
      "Epoch: 3000, loss: 0.6797512969553862\n",
      "Accuracy score: 0.5764356435643564\n",
      "Epoch: 3300, loss: 0.6907146836596273\n",
      "Accuracy score: 0.5724752475247525\n",
      "Epoch: 3600, loss: 0.6670357397098665\n",
      "Accuracy score: 0.5797029702970297\n",
      "Epoch: 3900, loss: 0.6651372696777432\n",
      "Accuracy score: 0.5654455445544556\n",
      "Epoch: 4200, loss: 0.6705423482790341\n",
      "Accuracy score: 0.5751485148514852\n",
      "Epoch: 4500, loss: 0.6837107149487331\n",
      "Accuracy score: 0.5736633663366337\n",
      "Epoch: 4800, loss: 0.7044428910685689\n"
     ]
    }
   ],
   "source": [
    "tuple_of_outcomes_emb = gru_train(epochs=5000, cell_size=cell_size, voc_size=50,batch_size=50,\n",
    "                                                   no_words=50,lr=0.01, Y=Y, array_x=df, Tx=Tx,reporting_freq=300,arr_or_df='df',\n",
    "                                                  normed_emb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- OK we seem to have the best results with the simple RNN and the word embedding. let's try that in a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 0.543\n"
     ]
    }
   ],
   "source": [
    "###do validation\n",
    "n=0\n",
    "k=5000\n",
    "x_val, mask_x_val = create_embed_batch(df_val.reset_index(),n,k,Tx,norm=False)\n",
    "y = Y_val[:,n:k]\n",
    "z_dict, a_dict, Yz, Ÿ = full_fw_prop(Tx = Tx, z_dict=z_dict, a_dict=a_dict, weights_dict = weights_dict, x=x_val,mask=mask_x_val)\n",
    "print(f'Validation score: {np.mean(y==np.where(Ÿ>0.5,1,0))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 90000)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7146337065083485"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.sum(Y_val*np.log(Ÿ) + (1-Y_val)*np.log(1-Ÿ)) / 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's compare these results to what we can achieve in tensorflow, including using an LSTM and stacked LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, RNN, GRU, SimpleRNN, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size=100000\n",
    "vocab_size = 400001\n",
    "emb_dim = 50\n",
    "Tx = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentences to array of indices\n",
    "df['words_by_idx'] = df['split_words'].apply(lambda x: np.array([words_to_index[y] for y in x if y in words_to_index.keys()]))\n",
    "\n",
    "#turn it into a big array\n",
    "x_array_keras = np.zeros((df_size,30))\n",
    "for idx, wordlist in enumerate(df['words_by_idx']):\n",
    "    for t in range(Tx):\n",
    "        if t < len(wordlist):\n",
    "            x_array_keras[idx,t] = wordlist[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now create embedding layer\n",
    "emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "for word, idx in words_to_index.items():\n",
    "    emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "#create embedding layer\n",
    "embedding_layer = Embedding(vocab_size,emb_dim,trainable=False)\n",
    "\n",
    "embedding_layer.build((None,)) # Do not modify the \"None\".  This line of code is complete as-is.\n",
    "    \n",
    "embedding_layer.set_weights([emb_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_indices = Input(shape=(Tx),dtype='int32')\n",
    "    \n",
    "# Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "\n",
    "embeddings = embedding_layer(sentence_indices)   \n",
    "\n",
    "X = SimpleRNN(units=120,return_sequences=False,kernel_initializer='glorot_uniform')(embeddings)\n",
    "\n",
    "X = Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform')(X)\n",
    "\n",
    "# Create Model instance which converts sentence_indices into X.\n",
    "model = Model(inputs=sentence_indices, outputs=X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 30, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 120)               20520     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 20,020,691\n",
      "Trainable params: 20,641\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 9s 10ms/step - loss: 0.6565 - accuracy: 0.6143 - val_loss: 0.6064 - val_accuracy: 0.6745\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6165 - accuracy: 0.6672 - val_loss: 0.6060 - val_accuracy: 0.6808\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6079 - accuracy: 0.6712 - val_loss: 0.5960 - val_accuracy: 0.6862\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.5977 - accuracy: 0.6823 - val_loss: 0.5878 - val_accuracy: 0.6741\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.5861 - accuracy: 0.6908 - val_loss: 0.5811 - val_accuracy: 0.6991\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.5824 - accuracy: 0.6988 - val_loss: 0.5958 - val_accuracy: 0.6836\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.5687 - accuracy: 0.7071 - val_loss: 0.5730 - val_accuracy: 0.6959\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 13s 16ms/step - loss: 0.5623 - accuracy: 0.7114 - val_loss: 0.5573 - val_accuracy: 0.7149\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 0.5586 - accuracy: 0.7155 - val_loss: 0.5671 - val_accuracy: 0.6980\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 8s 11ms/step - loss: 0.5496 - accuracy: 0.7224 - val_loss: 0.5459 - val_accuracy: 0.7235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fedbbccbbb0>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_array_keras,y=Y.reshape(-1,1),validation_split=0.2, batch_size=100,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "We've already beaten the handmade model! let's try a couple more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 27s 33ms/step - loss: 0.6340 - accuracy: 0.6215 - val_loss: 0.5608 - val_accuracy: 0.7090\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 27s 34ms/step - loss: 0.5471 - accuracy: 0.7226 - val_loss: 0.5580 - val_accuracy: 0.7058\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 25s 31ms/step - loss: 0.5158 - accuracy: 0.7439 - val_loss: 0.5054 - val_accuracy: 0.7506\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 26s 32ms/step - loss: 0.4935 - accuracy: 0.7610 - val_loss: 0.4918 - val_accuracy: 0.7646\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 25s 31ms/step - loss: 0.4773 - accuracy: 0.7735 - val_loss: 0.4867 - val_accuracy: 0.7683\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 26s 32ms/step - loss: 0.4622 - accuracy: 0.7818 - val_loss: 0.4786 - val_accuracy: 0.7713\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 25s 31ms/step - loss: 0.4506 - accuracy: 0.7892 - val_loss: 0.5069 - val_accuracy: 0.7639\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 27s 33ms/step - loss: 0.4356 - accuracy: 0.7989 - val_loss: 0.5107 - val_accuracy: 0.7528\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 26s 33ms/step - loss: 0.4196 - accuracy: 0.8079 - val_loss: 0.4837 - val_accuracy: 0.7717\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 26s 33ms/step - loss: 0.4051 - accuracy: 0.8160 - val_loss: 0.4912 - val_accuracy: 0.7691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fedc7da9df0>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GRU\n",
    "sentence_indices = Input(shape=(Tx),dtype='int32')\n",
    "embeddings = embedding_layer(sentence_indices)   \n",
    "X = GRU(units=120,return_sequences=False,kernel_initializer='glorot_uniform')(embeddings)\n",
    "X = Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform')(X)\n",
    "model_GRU = Model(inputs=sentence_indices, outputs=X)\n",
    "model_GRU.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics='accuracy')\n",
    "model_GRU.fit(x=x_array_keras,y=Y.reshape(-1,1),validation_split=0.2, batch_size=100,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 32s 38ms/step - loss: 0.6195 - accuracy: 0.6569 - val_loss: 0.5645 - val_accuracy: 0.7067\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 32s 39ms/step - loss: 0.5577 - accuracy: 0.7170 - val_loss: 0.5447 - val_accuracy: 0.7192\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 30s 37ms/step - loss: 0.5303 - accuracy: 0.7363 - val_loss: 0.5201 - val_accuracy: 0.7400\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 34s 42ms/step - loss: 0.5090 - accuracy: 0.7495 - val_loss: 0.5130 - val_accuracy: 0.7567\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 30s 38ms/step - loss: 0.4916 - accuracy: 0.7632 - val_loss: 0.5021 - val_accuracy: 0.7516\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4777 - accuracy: 0.7711 - val_loss: 0.4912 - val_accuracy: 0.7612\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 31s 39ms/step - loss: 0.4571 - accuracy: 0.7851 - val_loss: 0.4875 - val_accuracy: 0.7685\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 36s 45ms/step - loss: 0.4453 - accuracy: 0.7919 - val_loss: 0.5124 - val_accuracy: 0.7423\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 33s 41ms/step - loss: 0.4319 - accuracy: 0.8020 - val_loss: 0.4979 - val_accuracy: 0.7659\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 34s 42ms/step - loss: 0.4213 - accuracy: 0.8054 - val_loss: 0.4862 - val_accuracy: 0.7713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fedb9a82ca0>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM\n",
    "sentence_indices = Input(shape=(Tx),dtype='int32')\n",
    "embeddings = embedding_layer(sentence_indices)   \n",
    "X = LSTM(units=120,return_sequences=False,kernel_initializer='glorot_uniform')(embeddings)\n",
    "X = Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform')(X)\n",
    "model_LSTM = Model(inputs=sentence_indices, outputs=X)\n",
    "model_LSTM.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics='accuracy')\n",
    "model_LSTM.fit(x=x_array_keras,y=Y.reshape(-1,1),validation_split=0.2, batch_size=100,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Significantly better performance with Keras. This could be down to the optimizer, the initialization method, or it could be a mathematical error in the formula! \n",
    "\n",
    "GRU bas performed the best, with little overfitting. Let's add another stacked layer and see if we can get a further improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 35s 41ms/step - loss: 0.6307 - accuracy: 0.6369 - val_loss: 0.5702 - val_accuracy: 0.6987\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 29s 36ms/step - loss: 0.5638 - accuracy: 0.7036 - val_loss: 0.5460 - val_accuracy: 0.7188\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 38s 47ms/step - loss: 0.5387 - accuracy: 0.7223 - val_loss: 0.5284 - val_accuracy: 0.7297\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 35s 43ms/step - loss: 0.5226 - accuracy: 0.7350 - val_loss: 0.5494 - val_accuracy: 0.7161\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 35s 43ms/step - loss: 0.5089 - accuracy: 0.7435 - val_loss: 0.5220 - val_accuracy: 0.7349\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 32s 40ms/step - loss: 0.4967 - accuracy: 0.7529 - val_loss: 0.5199 - val_accuracy: 0.7410\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 33s 41ms/step - loss: 0.4840 - accuracy: 0.7606 - val_loss: 0.5099 - val_accuracy: 0.7472\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 33s 42ms/step - loss: 0.4690 - accuracy: 0.7698 - val_loss: 0.5116 - val_accuracy: 0.7440\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 37s 46ms/step - loss: 0.4580 - accuracy: 0.7771 - val_loss: 0.5117 - val_accuracy: 0.7465\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 33s 41ms/step - loss: 0.4463 - accuracy: 0.7847 - val_loss: 0.5155 - val_accuracy: 0.7424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fedb9031280>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_indices = Input(shape=(Tx),dtype='int32')\n",
    "embeddings = embedding_layer(sentence_indices)   \n",
    "X = GRU(units=120,return_sequences=True,kernel_initializer='glorot_uniform')(embeddings)\n",
    "X = GRU(units=64,return_sequences=True,kernel_initializer='glorot_uniform')(X)\n",
    "X = Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform')(X)\n",
    "model_GRU = Model(inputs=sentence_indices, outputs=X)\n",
    "model_GRU.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics='accuracy')\n",
    "model_GRU.fit(x=x_array_keras,y=Y.reshape(-1,1),validation_split=0.2, batch_size=100,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It doesn't improve things.\n",
    "\n",
    "The next steps will be to try some hyperparameter tuning and increase the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
