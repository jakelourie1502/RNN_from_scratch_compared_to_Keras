{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>polarity</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179646</th>\n",
       "      <td>b'Fri May 01 21:05:41 PDT 2009'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b\"I just saw Taylor Swift's new music video 4 ...</td>\n",
       "      <td>b'Kylamvjonasfan'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94779</th>\n",
       "      <td>b'Mon Jun 15 00:14:12 PDT 2009'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'Bk ish was jumpin  saw da homie @flyestoncam...</td>\n",
       "      <td>b'image_esh'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   date  polarity        query  \\\n",
       "179646  b'Fri May 01 21:05:41 PDT 2009'         4  b'NO_QUERY'   \n",
       "94779   b'Mon Jun 15 00:14:12 PDT 2009'         4  b'NO_QUERY'   \n",
       "\n",
       "                                                     text               user  \n",
       "179646  b\"I just saw Taylor Swift's new music video 4 ...  b'Kylamvjonasfan'  \n",
       "94779   b'Bk ish was jumpin  saw da homie @flyestoncam...       b'image_esh'  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_size = 250000\n",
    "ds = tfds.load('sentiment140', split='train', shuffle_files=True)\n",
    "df = tfds.as_dataframe(ds.take(df_size))\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    125116\n",
       "0    124884\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Note: Binary classification can be used. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bin_polarity'] = df['polarity'].apply(lambda x: 1 if x == 4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the RNN is that we take just the output of the final word (unless we're doing a bidirectional format, or a concatenated format where we concatenate the output of all the words)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating X values\n",
    "\n",
    "The text is in byte form, so we need to convert it to string form, and then use the split functionality to convert to a list. We ignore the first two characters which are a side effect of the conversion, and the last element in the string which is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns names: ['date', 'polarity', 'query', 'text', 'user', 'bin_polarity', 'split_words', 'txt_length']\n"
     ]
    }
   ],
   "source": [
    "print(f'columns names: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separating the words into a list of words\n",
    "df['split_words'] = df['text'].apply(lambda x: str(x)[2:].split()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the length of words\n",
    "df['txt_length'] = df['split_words'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.7, 11.4]      81665\n",
       "(11.4, 17.1]     61583\n",
       "(17.1, 22.8]     39162\n",
       "(-0.058, 5.7]    38476\n",
       "(22.8, 28.5]     27616\n",
       "(28.5, 34.2]      1494\n",
       "(34.2, 39.9]         3\n",
       "(51.3, 57.0]         1\n",
       "(45.6, 51.3]         0\n",
       "(39.9, 45.6]         0\n",
       "Name: txt_length, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['txt_length'].value_counts(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Very few tweets are over 30 words, so we will create an RNN based on 30 words\n",
    "\n",
    "We now need to create a words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 30 #this is the value we use to specify how many words to consider in the tweet (first e.g. 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create words dict'''\n",
    "\n",
    "no_words = 5000\n",
    "def create_words_dict(df,no_words):\n",
    "    \n",
    "    top_words = list(df['split_words'].explode().value_counts()[:no_words].index)\n",
    "    word_dict = {}\n",
    "    for idx, word in enumerate(top_words):\n",
    "        word_dict[idx+1] = word\n",
    "\n",
    "    #create additional values for 'word is over' and 'unknown'\n",
    "    word_dict[no_words+1] = 'word_over'\n",
    "    word_dict[0] = 'UNKNOWN-WORD'\n",
    "\n",
    "    #create reversed dict\n",
    "    word_dict_reversed = {}\n",
    "    for key, value in word_dict_2k.items():\n",
    "        word_dict_reversed[value] = key\n",
    "    \n",
    "    return word_dict, word_dict_reversed\n",
    "word_dict_2k, word_dict_reversed = create_words_dict(df,no_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Our X values need to be vocab_size x 30 x m, one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the words in numeric index format\n",
    "    \n",
    "def create_x_array(df,Tx):\n",
    "    x = df['split_words'].apply(lambda x: np.array([word_dict_reversed[word] if word in word_dict_reversed.keys() else 0 for word in x]))\n",
    "\n",
    "    '''we're going to make all the x values the same length'''\n",
    "\n",
    "    array_x = np.zeros((df_size,Tx)) #blank array to put them in\n",
    "    for idx,arr in enumerate(x):\n",
    "        leng = len(arr) \n",
    "        #get length, and fork based on current size\n",
    "        if leng > Tx:\n",
    "            array_x[idx] = arr[:Tx] #just take first 30 values\n",
    "        elif leng < Tx:\n",
    "            array_x[idx] = np.append(arr,np.zeros(Tx-leng)+no_words+1) #append 30 minus the current length -1s\n",
    "    #put examples on the columns\n",
    "    array_x = array_x.T\n",
    "    return array_x,x\n",
    "\n",
    "\n",
    "array_x,x = create_x_array(df,Tx=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 250000)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now one hot encode them'''\n",
    "def one_hot_process(array_x,Tx, batch_size):\n",
    "    one_hot_x_40k = np.zeros((no_words+2,Tx,batch_size))\n",
    "    for row_idx, row in enumerate(array_x[:,:batch_size]):\n",
    "        for exam_idx, word_val in enumerate(row):\n",
    "            one_hot_x_40k[int(word_val),row_idx,exam_idx] = 1\n",
    "    return one_hot_x_40k\n",
    "def mask_process(array_x, batch_size):\n",
    "    return np.where(array_x[:,:batch_size]==no_words+1,1,0)\n",
    "def one_hot_and_mask(array_x, Tx, batch_size):\n",
    "    one_hot_x_40k = one_hot_process(array_x,Tx, batch_size)\n",
    "    mask_x = mask_process(array_x, batch_size)\n",
    "    return one_hot_x_40k, mask_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_x_40k = one_hot_process(array_x,Tx, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "We've used 2002 in order to incorporatate the 2001th word (end of word) and unknown index. We can need to create a mask which tells the machine to skip if the mask is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_x  = mask_process(array_x, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 10000)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn Y into an array\n",
    "Y = np.array(reduced_df['bin_polarity']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "OK so we have our x inputs. Mask x, shaped 30 x 40000, and the actual onehotencoded x values, of shape 2001, 30, 40000\n",
    "\n",
    "In the basic RNN, ignoring the last stage for a second, each cell has three weights matrixes.\n",
    "\n",
    "### Forward Prop\n",
    "\n",
    "WAa - the weights applied to the previous cells outputs\n",
    "\n",
    "WAx - the weights applied to the X values\n",
    "\n",
    "WaB - a bias term. \n",
    "\n",
    "We have to make a choice of how large each cell is, and then initialise the weights. We'll use a Xavier initialization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a cell size of 50\n",
    "cell_size = 50\n",
    "x_size = no_words+no_words #Xavier_initialization of weights.\n",
    "WAa = np.random.uniform(-(6)/(np.sqrt(cell_size*2)),(6)/((cell_size*2)),[cell_size,cell_size]) \n",
    "WAb = np.random.uniform(-(6)/(np.sqrt(cell_size+1)),(6)/((cell_size+1)),[cell_size,1]) \n",
    "WAx = np.random.uniform(-(6)/(np.sqrt(cell_size+x_size)),(6)/((cell_size+x_size)),[cell_size,x_size]) \n",
    "\n",
    "#We need a weights matrix too for later\n",
    "WYa = np.random.uniform(-(6)/((cell_size+1)),(6)/((cell_size+1)),[1,cell_size])\n",
    "WYb = np.random.uniform(-(6)/((1+1)),(6)/((1+1)),[1,1])\n",
    "\n",
    "weights_dict = {}\n",
    "weights_dict['WAa'] = WAa\n",
    "weights_dict['WAb'] = WAb\n",
    "weights_dict['WAx'] = WAx\n",
    "weights_dict['WYa'] = WYa\n",
    "weights_dict['WYb'] = WYb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's implement one run of forward prop in order to replicate it going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these allow us to set some default values\n",
    "a0 = np.zeros((cell_size,df_size))\n",
    "z_dict, a_dict = {}, {}\n",
    "a_dict[0] = a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''See notes from above - this is just implemented as a function'''\n",
    "def forward_prop(a_prev,  WAa, WAx, WAb, x, mask,time_period=0):\n",
    "    \n",
    "    # #z1 - the pre-tanh values of a0, bias and x1 times weights\n",
    "    z1 = WAa @ a_prev + WAx @ x[:,time_period,:] + WAb\n",
    "    \n",
    "    # #a1 either equals a1 if mask ==1 or equal tanh z1 if mask == - \n",
    "    a1 = mask[time_period].reshape(1,-1) * a_prev + (1-mask[time_period]) * np.tanh(z1)\n",
    "    \n",
    "    return a1, z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The idea here is to loop over the time periods and run forward prop each time. We will need to re-use a1 and z1 when we come back and do the back prop, so we will store them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cells_fw_prop(Tx, z_dict, a_dict, a0,WAa, WAx, WAb, x, mask):\n",
    "    a_prev = a0 #to have an initial value to call the forward_prop function\n",
    "    for i in range(Tx):\n",
    "        a_, z_ = forward_prop(a_prev=a_prev, time_period = i,x=x,mask = mask,WAa=WAa, WAx=WAx, WAb=WAb)\n",
    "        z_dict[i+1], a_dict[i+1] = z_, a_\n",
    "        a_prev = a_\n",
    "    #we end by returning the dictionary for z and a values throughout the time period\n",
    "    return z_dict, a_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Lets put it all together in a forward prop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Yz):\n",
    "        return (1/(1+np.exp(-Yz)))\n",
    "    \n",
    "def full_fw_prop(weights_dict, z_dict, a_dict, x, mask, Tx = Tx):\n",
    "    \n",
    "    # initialize a0 and dict values\n",
    "    a0 = np.zeros((cell_size, x.shape[2]))\n",
    "    a_dict[0] = a0\n",
    "    WAa, WAb, WAx, WYa, WYb = weights_dict.values()\n",
    "\n",
    "    #do the cell forward prop    \n",
    "    z_dict, a_dict = cells_fw_prop(Tx = Tx, z_dict=z_dict, a_dict=a_dict, a0=a0,WAa=WAa, WAx=WAx, WAb=WAb, x=x,mask=mask)\n",
    "\n",
    "    #extract last value and prediction\n",
    "    final_a = a_dict[Tx]\n",
    "    Yz = WYa @ final_a + WYb\n",
    "   \n",
    "    Ÿ = sigmoid(Yz)\n",
    "\n",
    "    return z_dict, a_dict, Yz, Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Back Propogation\n",
    "\n",
    "We can conveniently use the differentiated sigmoid function, which gives\n",
    "\n",
    "DL/DZ = Ÿ - Y\n",
    "\n",
    "From there we can initially back calculate the values of the final portion, giving values for DL/DA, DL/DWYa, DL/DWYb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn Y into an array\n",
    "Y = np.array(reduced_df['bin_polarity']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start by initializing the backprop dicts\n",
    "dA_dict, dZ_dict = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "OK let's create the relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_one_layer(prev_Loss, t, weights_dict,  a_dict,z_dict,  X, mask_x,learning_rate = 0.01,batch_size=400):\n",
    "    \n",
    "    WAa, WAb, WAx, WYa, WYb = weights_dict.values()\n",
    "    a_ = a_dict[t]\n",
    "    z_ = z_dict[t]\n",
    "    a__ = a_dict[t-1]\n",
    "    x_ = X[:,t-1,:]\n",
    "\n",
    "    #get dA and dZ as a base for the layer\n",
    "    dL_dA = prev_Loss #40000,50\n",
    "    dA_dZa = ((1-mask_x[t-1]) * (1-(np.tanh(z_)**2))).T #40000,50\n",
    "    \n",
    "    #get a previous\n",
    "    dZ_dAp = WAa #50x50\n",
    "    dA_dAp = mask_x[t-1].reshape(-1,1) + np.zeros_like((dL_dA)) + (dA_dZa @ dZ_dAp) #we create a mask of 1s everywhere here.\n",
    "    dL_dAp = dL_dA * dA_dAp\n",
    "\n",
    "    #differentiate with respect to weights\n",
    "    dZ_dWAa = a__ #50 x m\n",
    "    dL_dWAa = ((dL_dA * dA_dZa).T @ dZ_dWAa.T)/batch_size #weights dont matter if we are having a 0 on the mask\n",
    "\n",
    "    dZ_dWAb = np.zeros((batch_size,1))+1\n",
    "    dL_dWAb = ((dL_dA * dA_dZa).T @ dZ_dWAb)/batch_size\n",
    "\n",
    "    dZ_dWAx = x_\n",
    "    dL_dWAx = ((dL_dA * dA_dZa).T @ dZ_dWAx.T)/batch_size #weights dont matter if we are having a 0 on the mask\n",
    "    \n",
    "    #now update the weights based on the findings here.\n",
    "    return dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_full_swing(Ÿ, Y, Tx, weights_dict,  a_dict,z_dict,  X, mask_x,learning_rate = 0.01,batch_size = 100):\n",
    "    \n",
    "    '''initial loss function w.r.t Z'''\n",
    "    WAa, WAb, WAx, WYa, WYb = weights_dict.values()\n",
    "    dL_dZ = (Ÿ - Y).T #40000 x 1\n",
    "\n",
    "\n",
    "    # first order diffs\n",
    "    dZ_dA = WYa # 1 x 50\n",
    "    dZ_dWYa = a_dict[Tx] # 50 x 40000\n",
    "    dZ_dB = np.zeros_like(dL_dZ) + 1\n",
    "\n",
    "    #chain ruled diffs\n",
    "    dL_dA = dL_dZ @ dZ_dA\n",
    "    dL_dWYa = (dZ_dWYa @ dL_dZ).T / batch_size\n",
    "    dL_dWYb = (dZ_dB.T @ dL_dZ)/ batch_size\n",
    "    \n",
    "    weights_dict['WYa'] -= learning_rate * dL_dWYa\n",
    "    weights_dict['WYb'] -= learning_rate * dL_dWYb\n",
    "    \n",
    "    prev_loss = dL_dA\n",
    "    '''now go through the other functions'''\n",
    "    for t in reversed(range(1,Tx)):\n",
    "        #extract relevant differentials for updating backprop and also carrying on the backprop through the layers\n",
    "        dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp = back_prop_one_layer(prev_Loss=prev_loss, t=t, weights_dict = weights_dict,X=x,  a_dict=a_dict,z_dict=z_dict,mask_x=mask_x,batch_size=batch_size)\n",
    "        weights_dict['WAa'] -= learning_rate*dL_dWAa\n",
    "        weights_dict['WAx'] -= learning_rate*dL_dWAx\n",
    "        weights_dict['WAb'] -= learning_rate*dL_dWAb\n",
    "        prev_loss = dL_dAp\n",
    "        return dL_dZ, dZ_dA, dZ_dB, dL_dA, dL_dWYa, dL_dWYb, dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back_prop_full_swing(Ÿ=Ÿ, Y=Y, Tx=Tx, weights_dict=weights_dict,  a_dict=a_dict,z_dict=z_dict,  X=one_hot_x_40k, mask_x=mask_x,learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Putting it all together\n",
    "\n",
    "Let's now create a function which runs forward prop and backward prop in batches, and calculates the loss at each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.41333333333333333\n",
      "Epoch: 0, loss: 0.8744233243901126\n",
      "Accuracy score: 0.5210526315789474\n",
      "Epoch: 200, loss: 0.6743788061355299\n",
      "Accuracy score: 0.5291228070175439\n",
      "Epoch: 400, loss: 0.6454877514690217\n",
      "Accuracy score: 0.5456140350877193\n",
      "Epoch: 600, loss: 0.6587351042883004\n",
      "Accuracy score: 0.5505263157894738\n",
      "Epoch: 800, loss: 0.6892139057242007\n"
     ]
    }
   ],
   "source": [
    "'''set some initial parameters'''\n",
    "epochs = 2001\n",
    "cell_size = 200\n",
    "x_size = no_words+2\n",
    "\n",
    "''' initialize weights'''\n",
    "\n",
    "WAx = np.random.uniform(-1,1,[cell_size,x_size])\n",
    "WAb = np.random.uniform(-1,1,[cell_size,1])\n",
    "WAa = np.random.uniform(-0.1,0.1,[cell_size,cell_size])\n",
    "WYa = np.random.uniform(-0.25,0.25,[1,cell_size])\n",
    "WYb = np.random.uniform(-1,1,[1,1])\n",
    "\n",
    "weights_dict = {}\n",
    "weights_dict['WAa'] = WAa\n",
    "weights_dict['WAb'] = WAb\n",
    "weights_dict['WAx'] = WAx\n",
    "weights_dict['WYa'] = WYa\n",
    "weights_dict['WYb'] = WYb\n",
    "\n",
    "'''define additional dictionaries and characteristics'''\n",
    "z_dict, a_dict = {}, {}\n",
    "losses = []\n",
    "accs = []\n",
    "batches=2000\n",
    "batch_size = int(df_size//batches) #so 400\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #run fw prop\n",
    "    '''create batching'''\n",
    "    \n",
    "\n",
    "    ''' This sections deals with batching'''\n",
    "    \n",
    "    round_ = epoch % batches #deals with if it's the first of a new set of batches\n",
    "    n, k = batch_size*round_, batch_size*(1 + round_)\n",
    "    x = one_hot_x_40k[:,:,n:k]\n",
    "    mask_x_ = mask_x[:,n:k]\n",
    "    y = Y[0,n:k]\n",
    "    \n",
    "    '''Forward prop'''\n",
    "    z_dict, a_dict, Yz, Ÿ = full_fw_prop(Tx = Tx, z_dict=z_dict, a_dict=a_dict, weights_dict = weights_dict, x=x,mask=mask_x_)\n",
    "    '''create loss metrics for reporting'''\n",
    "    loss = -np.sum(y*np.log(Ÿ) + (1-y)*np.log(1-Ÿ))/batch_size\n",
    "    accuracy = np.sum(y==np.where(Ÿ>0.5,1,0))/batch_size\n",
    "    losses.append(loss)\n",
    "    accs.append(accuracy)\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'Accuracy score: {np.mean(accs[(epoch-batch_size):])}')    \n",
    "        print(f'Epoch: {epoch}, loss: {loss}')\n",
    "    \n",
    "    #run backprop\n",
    "    dL_dZ, dZ_dA, dZ_dB, dL_dA, dL_dWYa, dL_dWYb, dL_dWAa, dL_dWAx, dL_dWAb, dL_dAp = back_prop_full_swing(Ÿ=Ÿ, Y=y, Tx=Tx, weights_dict=weights_dict,  a_dict=a_dict,z_dict=z_dict,  X=x, mask_x=mask_x_,learning_rate = 0.1,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-374973eefd04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a_dict' is not defined"
     ]
    }
   ],
   "source": [
    "a_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We see a small amount of ability from the model to predict sentiment.\n",
    "\n",
    "#### GRU\n",
    "\n",
    "We'll use the same format, albeit with different functions to build a GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Weights initialisation'''\n",
    "def weights_initialisation(batch_size,cell_size=50,x_size=2002):\n",
    "    #initialise the weights for the main 'estimator' node\n",
    "    weights={}\n",
    "    weights['gWCx'] = np.random.uniform(-1,1,[cell_size, x_size])\n",
    "    weights['gWCc'] = np.random.uniform(-1,1,[cell_size, cell_size])\n",
    "    weights['gWCb'] = np.random.uniform(-1,1,[cell_size,1])\n",
    "\n",
    "    #initialize weights for the 'update' node\n",
    "    weights['gWUx'] = np.random.uniform(-1,1,[cell_size, x_size])\n",
    "    weights['gWUc'] = np.random.uniform(-1,1,[cell_size, cell_size])\n",
    "    weights['gWUb'] = np.random.uniform(-1,1,[cell_size,1])\n",
    "\n",
    "    #initalise weights for output layer\n",
    "    weights['gWYa'] = np.random.uniform(-0.25,0.25,[1,cell_size])\n",
    "    weights['gWYb'] = np.random.uniform(-1,1,[1,1])\n",
    "    \n",
    "    a0 = np.zeros((cell_size, batch_size)) #create blank a0\n",
    "    return weights, a0\n",
    "\n",
    "def dict_initialisation_gru():\n",
    "    zc_dict, zu_dict, č_dict, cupd_dict, c_dict = {}, {}, {}, {}, {}\n",
    "    return zc_dict, zu_dict, č_dict, cupd_dict, c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gforward_prop_1(c_prev, weights, x, mask, t=0):\n",
    "    gWCx, gWCc, gWCb, gWUx, gWUc, gWUb, _, _ = weights.values()\n",
    "    \n",
    "    #create value for č by creating a z value to be tanhd + b\n",
    "    \n",
    "    zc = gWCx @ x[:,t,:] + gWCc @ c_prev + gWCb\n",
    "    č = np.tanh(zc)\n",
    "    \n",
    "    #now create a value for c_update, and use mask to make this 0 if the mask is on.\n",
    "    zu = gWUx @ x[:,t,:] + gWUc @ c_prev + gWUb \n",
    "    Cupd = (1-mask[t]) * sigmoid(zu) #this will be zero if mask is 1\n",
    "    \n",
    "    #now update c\n",
    "    c = Cupd * č + (1-Cupd) * c_prev\n",
    "    \n",
    "    return zc, č, zu, Cupd, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcells_fw_prop(zc_dict, zu_dict, č_dict, cupd_dict, c_dict, a0, weights, x, mask,Tx):\n",
    "    \n",
    "    c_prev = a0 #to have an initial value to call the forward_prop function\n",
    "    for t in range(Tx):\n",
    "        zc, č, zu, Cupd, c = gforward_prop_1(c_prev=c_prev, t = t,x=x,mask = mask, weights=weights)\n",
    "        zc_dict[t+1], č_dict[t+1], zu_dict[t+1], cupd_dict[t+1], c_dict[t+1] = zc, č, zu, Cupd, c\n",
    "        c_prev = c\n",
    "    #we end by returning the dictionary for z and a values throughout the time period\n",
    "    return zc_dict, zu_dict, č_dict, cupd_dict, c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfull_fw_prop(a0, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask, Tx = Tx):\n",
    "    \n",
    "    #do the cell forward prop    \n",
    "    zc_dict, zu_dict, č_dict, cupd_dict, c_dict = gcells_fw_prop(zc_dict, zu_dict, č_dict, cupd_dict, c_dict, a0, weights, x, mask,Tx)\n",
    "\n",
    "    #extract last value and prediction\n",
    "    final_c = c_dict[Tx]\n",
    "    Yz = weights['gWYa'] @ final_c + weights['gWYb']\n",
    "    Ÿ = sigmoid(Yz)\n",
    "\n",
    "    return zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Yz):\n",
    "        return (1/(1+np.exp(-Yz)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002, 30, 10000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_x_40k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.005293171860522315\n",
      "Variance: 0.5036911379840215\n"
     ]
    }
   ],
   "source": [
    "#when i initialise the weights, with a different x size, gWCx gives that value. same with cell size\n",
    "x_size_ut = 5002\n",
    "c_size_ut = 50\n",
    "batch_size= 1000\n",
    "x_ut, mask_x_ut = one_hot_x_40k[:,:,:batch_size], mask_x[:,:batch_size]\n",
    "y_ut = Y[:,:batch_size]\n",
    "\n",
    "weights_ut,a0= weights_initialisation(cell_size=c_size_ut,x_size=x_size_ut,batch_size=batch_size)\n",
    "assert weights_ut['gWUx'].shape == (c_size_ut, x_size_ut)\n",
    "\n",
    "#when i call the main function, c_dict[30] should be numbers with mean ~ 0 and var [0.25,1.25]\n",
    "zc_dict, zu_dict, č_dict, cupd_dict, c_dict = dict_initialisation_gru()\n",
    "zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ = gfull_fw_prop(a0, weights_ut, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x_ut, mask_x_ut, Tx)\n",
    "\n",
    "assert abs(np.mean(c_dict[30])) < 0.1\n",
    "assert 0.25 < np.var(c_dict[30]) < 1.25\n",
    "\n",
    "print(f'Mean: {np.mean(c_dict[30])}\\nVariance: {np.var(c_dict[30])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Alright now we're burning diesel.\n",
    "\n",
    "Time for back prop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gback_prop_one_layer(prev_loss, t, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict,  x, mask_x,learning_rate = 0.01,batch_size=400):\n",
    "    \n",
    "    gWCx, gWCc, gWCb, gWUx, gWUc, gWUb, gWYa, gWYb = weights.values()\n",
    "    zc, zu, č, cupd, c = zc_dict[t], zu_dict[t], č_dict[t], cupd_dict[t], c_dict[t]\n",
    "    c_ = c_dict[t-1] #also take the last c value frm previous round for differentials\n",
    "    x_ = x[:,t-1,:] #relevant x values, 2000 x m\n",
    "\n",
    "    dl_dc = prev_loss #m,50\n",
    "\n",
    "    '''first backpropogate through to get c previous'''\n",
    "    dc_dč = cupd #50 x m \n",
    "    dc_dcupd = č - c_ #50 x m\n",
    "    dč_dzc = 1 - np.tanh(zc)**2 #50 x m\n",
    "    dcupd_dzu = sigmoid(zu) * (1-sigmoid(zu)) #50 x m\n",
    "    dzc_dc_1 = gWCc #50,50\n",
    "    dzu_dc_1 = gWUc #50,50\n",
    "\n",
    "    #1- cupd is 50, m\n",
    "    dc_dc_1 = (1-cupd).T + ((dc_dč * dč_dzc).T @ gWCc) + ((dc_dcupd * dcupd_dzu).T @ gWUc) #ends as 50 x m \n",
    "    dl_dc_1 = dl_dc * dc_dc_1 #m x 50\n",
    "\n",
    "    \n",
    "    '''now get the weights with respect to L'''\n",
    "    #first three terms are element wise because they are the ru term and the sigmoid term.\n",
    "\n",
    "    dL_dWCx = (dl_dc.T * dc_dč * dč_dzc) @ x_.T #50 x m, m x 2000 = 50 x 2000 (same shape as WCx)\n",
    "    dL_dWCb = ((dl_dc.T * dc_dč * dč_dzc) @ (np.zeros((batch_size,1))+1))/batch_size #50 x m, m x 1 = 50 x1\n",
    "    dL_dWCc = (dl_dc.T * dc_dč * dč_dzc) @ c_.T #50 x m, m x 50 = 50 x 50\n",
    "    dL_dWUx = (dl_dc.T * dc_dcupd * dcupd_dzu) @ x_.T\n",
    "    dL_dWUb = ((dl_dc.T * dc_dcupd * dcupd_dzu) @ (np.zeros((batch_size,1))+1))/batch_size #50 x m, m x 1 = 50 x1\n",
    "    dL_dWUc = (dl_dc.T * dc_dcupd * dcupd_dzu) @ c_.T #50 x m, m x 50 = 50 x 50\n",
    "    return dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002, 30, 1000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gback_prop_full_swing(Ÿ, Y, Tx, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask_x,learning_rate = 0.01,batch_size = 100):\n",
    "    \n",
    "    '''initial loss function w.r.t Z'''\n",
    "    gWCx, gWCc, gWCb, gWUx, gWUc, gWUb, gWYa, gWYb = weights.values()\n",
    "    dL_dZy = (Ÿ - Y).T #40000 x 1\n",
    "\n",
    "    # first order diffs\n",
    "    dZ_dC = gWYa # 1 x 50\n",
    "    dZ_dWYa = c_dict[Tx] # 50 x 40000\n",
    "    dZ_dB = np.zeros_like(dL_dZy) + 1\n",
    "\n",
    "    #chain ruled diffs\n",
    "    dL_dC = dL_dZy @ dZ_dC\n",
    "    dL_dWYa = (dZ_dWYa @ dL_dZy).T / batch_size\n",
    "    dL_dWYb = (dZ_dB.T @ dL_dZy)/ batch_size\n",
    "    \n",
    "    #update end weights.\n",
    "    weights['gWYa'] -= learning_rate * dL_dWYa\n",
    "    weights['gWYb'] -= learning_rate * dL_dWYb\n",
    "    \n",
    "    prev_loss = dL_dC\n",
    "    '''now go through the other functions'''\n",
    "    for t in reversed(range(1,int(Tx))):\n",
    "        #extract relevant differentials for updating backprop and also carrying on the backprop through the layers\n",
    "        dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1 = gback_prop_one_layer(prev_loss, t, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict,  x, mask_x,learning_rate = 0.01,batch_size=batch_size)\n",
    "        weights['gWCx'] -= learning_rate*dL_dWCx\n",
    "        weights['gWCc'] -= learning_rate*dL_dWCc\n",
    "        weights['gWCb'] -= learning_rate*dL_dWCb\n",
    "        weights['gWUx'] -= learning_rate*dL_dWUx\n",
    "        weights['gWUc'] -= learning_rate*dL_dWUc\n",
    "        weights['gWCb'] -= learning_rate*dL_dWUb\n",
    "        prev_loss = dl_dc_1\n",
    "        return dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1, dL_dWYa,dL_dWYb, dL_dZy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Unit tests\n",
    "\n",
    "Manual: check that the top 4 values are 10%+ different versus the bottom 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.88170927233705 0.274305851214951\n",
      "0.33313811546797917 0.0192547977943042\n",
      "216.69760353753261 0.21641270161579887\n",
      "0.3331392838193671 0.02130430017008906\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(weights_ut['gWCx']),np.sum(weights_ut['gWYa']))\n",
    "print(np.var(weights_ut['gWCx']),np.var(weights_ut['gWYa']))\n",
    "for i in range(10):\n",
    "    dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1, dL_dWYa,dL_dWYb, dL_dZy = gback_prop_full_swing(Ÿ, y_ut, Tx, weights_ut, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x_ut, mask_x_ut,learning_rate = 0.1,batch_size = 1000)\n",
    "print(np.sum(weights_ut['gWCx']),np.sum(weights_ut['gWYa']))\n",
    "print(np.var(weights_ut['gWCx']),np.var(weights_ut['gWYa']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Putting it all together with the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set epochs, cell_size, x_size (from no_words)\n",
    "epochs = 50\n",
    "cell_size= 300\n",
    "voc_siz = no_words+2\n",
    "batch_size = 250\n",
    "lr = 0.05\n",
    "\n",
    "def gru_train(epochs, cell_size, voc_size,batch_size,no_words,lr, Y, array_x, Tx):\n",
    "    # initialize weights\n",
    "    weights,a0= weights_initialisation(cell_size=cell_size,x_size=voc_siz,batch_size=batch_size)\n",
    "    zc_dict, zu_dict, č_dict, cupd_dict, c_dict = dict_initialisation_gru()\n",
    "\n",
    "    #create loss & accuracy lists\n",
    "    glosses = []\n",
    "    gaccs = []\n",
    "\n",
    "    #create batches\n",
    "    batches = int(df_size//batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        #create batch of x, x mask and y\n",
    "        round_ = epoch % batches \n",
    "        n, k = batch_size*round_, batch_size*(1 + round_)\n",
    "        x, mask_x_ = one_hot_and_mask(array_x[:,n:k],Tx,batch_size)\n",
    "        y = Y[0,n:k]\n",
    "\n",
    "        #forward prop and loss calculation\n",
    "        zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ = gfull_fw_prop(a0, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask_x_, Tx)\n",
    "        '''create loss metrics for reporting'''\n",
    "        loss = -np.sum(y*np.log(Ÿ) + (1-y)*np.log(1-Ÿ))/batch_size\n",
    "        accuracy = np.sum(y==np.where(Ÿ>0.5,1,0))/batch_size\n",
    "        glosses.append(loss); gaccs.append(accuracy)\n",
    "        if epoch % 200 == 0:\n",
    "            print(f'Accuracy score: {np.mean(gaccs[(epoch-batch_size):])}')    \n",
    "            print(f'Epoch: {epoch}, loss: {loss}')\n",
    "\n",
    "        #back prop\n",
    "        dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1, dL_dWYa,dL_dWYb, dL_dZy = gback_prop_full_swing(Ÿ, y, Tx, weights, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, x, mask_x,learning_rate = lr,batch_size = batch_size)\n",
    "    \n",
    "    return glosses, gaccs, zc_dict, zu_dict, č_dict, cupd_dict, c_dict, Yz, Ÿ, dL_dWCx, dL_dWCb, dL_dWCc, dL_dWUx, dL_dWUb, dL_dWUc, dl_dc_1, dL_dWYa,dL_dWYb, dL_dZy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_of_outcomes = gru_train(epochs, cell_size, voc_siz,batch_size,no_words,lr, Y, array_x, Tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdb2dcb2e50>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(glosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>polarity</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>bin_polarity</th>\n",
       "      <th>split_words</th>\n",
       "      <th>txt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189562</th>\n",
       "      <td>b'Wed Jun 03 02:05:40 PDT 2009'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'@Shikha_M yeah do...but its hacked '</td>\n",
       "      <td>b'Pro_94JBIT'</td>\n",
       "      <td>0</td>\n",
       "      <td>[@Shikha_M, yeah, do...but, its, hacked]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75941</th>\n",
       "      <td>b'Mon Jun 15 15:14:00 PDT 2009'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'@radadams Do want '</td>\n",
       "      <td>b'Mayeh'</td>\n",
       "      <td>0</td>\n",
       "      <td>[@radadams, Do, want]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88864</th>\n",
       "      <td>b'Sun Jun 21 01:14:59 PDT 2009'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b\"I don't get it at all \"</td>\n",
       "      <td>b'jodeeluv'</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, don't, get, it, at, all]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>b'Fri May 29 12:41:21 PDT 2009'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'@PixyStixKitten The Last Unicorn...one of my...</td>\n",
       "      <td>b'rangerlyn'</td>\n",
       "      <td>1</td>\n",
       "      <td>[@PixyStixKitten, The, Last, Unicorn...one, of...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157169</th>\n",
       "      <td>b'Sat Jun 06 22:57:06 PDT 2009'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'listening to anberlin and passsing out '</td>\n",
       "      <td>b'ticklemexmik'</td>\n",
       "      <td>1</td>\n",
       "      <td>[listening, to, anberlin, and, passsing, out]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   date  polarity        query  \\\n",
       "189562  b'Wed Jun 03 02:05:40 PDT 2009'         0  b'NO_QUERY'   \n",
       "75941   b'Mon Jun 15 15:14:00 PDT 2009'         0  b'NO_QUERY'   \n",
       "88864   b'Sun Jun 21 01:14:59 PDT 2009'         0  b'NO_QUERY'   \n",
       "39095   b'Fri May 29 12:41:21 PDT 2009'         4  b'NO_QUERY'   \n",
       "157169  b'Sat Jun 06 22:57:06 PDT 2009'         4  b'NO_QUERY'   \n",
       "\n",
       "                                                     text             user  \\\n",
       "189562             b'@Shikha_M yeah do...but its hacked '    b'Pro_94JBIT'   \n",
       "75941                               b'@radadams Do want '         b'Mayeh'   \n",
       "88864                           b\"I don't get it at all \"      b'jodeeluv'   \n",
       "39095   b'@PixyStixKitten The Last Unicorn...one of my...     b'rangerlyn'   \n",
       "157169         b'listening to anberlin and passsing out '  b'ticklemexmik'   \n",
       "\n",
       "        bin_polarity                                        split_words  \\\n",
       "189562             0           [@Shikha_M, yeah, do...but, its, hacked]   \n",
       "75941              0                              [@radadams, Do, want]   \n",
       "88864              0                       [I, don't, get, it, at, all]   \n",
       "39095              1  [@PixyStixKitten, The, Last, Unicorn...one, of...   \n",
       "157169             1      [listening, to, anberlin, and, passsing, out]   \n",
       "\n",
       "        txt_length  \n",
       "189562           5  \n",
       "75941            3  \n",
       "88864            6  \n",
       "39095            7  \n",
       "157169           6  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "##### Struggling to get a strong signal, but it's worth noting that a lot of the comments are difficult to classify even for a human \n",
    "\n",
    "Let's try another datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=False)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = tfds.as_dataframe(train_dataset.take(df_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text'], dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"This was an absolutely terrible movie. Don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>b'I have been known to fall asleep during film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Mann photographs the Alberta Rocky Mountains...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  b\"This was an absolutely terrible movie. Don't...\n",
       "1      0  b'I have been known to fall asleep during film...\n",
       "2      0  b'Mann photographs the Alberta Rocky Mountains..."
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_words_mov = 5000\n",
    "## split words\n",
    "df2['split_words'] = df2['text'].apply(lambda x: str(x)[2:].split()[:-1])\n",
    "\n",
    "#create dictionary\n",
    "word_dict_mov, word_dict_reversed_mov = create_words_dict(df2,no_words_mov)\n",
    "\n",
    "#create x values\n",
    "array_x_mov,x_mov = create_x_array(df2,Tx_mov)\n",
    "\n",
    "#Turn Y into an array\n",
    "Y_mov = np.array(df2['label']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_mov = 2000 #5000\n",
    "lr_mov = 0.05\n",
    "batch_size_mov = 250\n",
    "cell_size_mov = 200\n",
    "Tx_mov = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.5\n",
      "Epoch: 0, loss: 1.1288098771217552\n"
     ]
    }
   ],
   "source": [
    "tuple_of_outcomes_mov = gru_train(epochs_mov, cell_size_mov, no_words_mov,batch_size_mov,no_words,lr_mov, Y_mov, array_x_mov, Tx_mov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x__, mask_x___ = one_hot_and_mask(array_x_mov[:,n:k],Tx_mov,batch_size_mov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 250)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_x___.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
